{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb84304f-4fa9-4d64-808e-c672d357ae6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Scanning MURA dataset directory: MURA-v1.1/train...\n",
      "Found 36812 images with labels\n",
      "Classifying images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 36812/36812 [00:00<00:00, 1166547.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully classified 36812 images into classes\n",
      "Class distribution:\n",
      "XR_WRIST_positive: 3987 images\n",
      "XR_WRIST_negative: 5765 images\n",
      "XR_SHOULDER_positive: 4169 images\n",
      "XR_SHOULDER_negative: 4211 images\n",
      "XR_HAND_positive: 1484 images\n",
      "XR_HAND_negative: 4059 images\n",
      "XR_FOREARM_positive: 661 images\n",
      "XR_FOREARM_negative: 1164 images\n",
      "XR_FINGER_positive: 1970 images\n",
      "XR_FINGER_negative: 3138 images\n",
      "XR_ELBOW_positive: 2007 images\n",
      "XR_ELBOW_negative: 2925 images\n",
      "XR_HUMERUS_positive: 599 images\n",
      "XR_HUMERUS_negative: 673 images\n",
      "Scanning MURA dataset directory: MURA-v1.1/valid...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3197 images with labels\n",
      "Classifying images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 3197/3197 [00:00<00:00, 1305157.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully classified 3197 images into classes\n",
      "Class distribution:\n",
      "XR_WRIST_positive: 295 images\n",
      "XR_WRIST_negative: 364 images\n",
      "XR_SHOULDER_positive: 278 images\n",
      "XR_SHOULDER_negative: 285 images\n",
      "XR_HAND_positive: 189 images\n",
      "XR_HAND_negative: 271 images\n",
      "XR_FOREARM_positive: 151 images\n",
      "XR_FOREARM_negative: 150 images\n",
      "XR_FINGER_positive: 247 images\n",
      "XR_FINGER_negative: 214 images\n",
      "XR_ELBOW_positive: 230 images\n",
      "XR_ELBOW_negative: 235 images\n",
      "XR_HUMERUS_positive: 140 images\n",
      "XR_HUMERUS_negative: 148 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_17060\\2273154682.py:484: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  autoencoder.load_state_dict(torch.load(checkpoint_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded autoencoder checkpoint from checkpoints-BAGAN-GP-WGAN-GP_Old-2/best_autoencoder.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_17060\\2273154682.py:244: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint from checkpoints-BAGAN\\checkpoint_epoch_41.pth, resuming from epoch 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/200: 100%|████████████████████████████████████████████████████████████████| 1151/1151 [21:08<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [42/200], D Loss: 0.6802, G Loss: 2.5148\n",
      "Checkpoint saved at checkpoints-BAGAN\\checkpoint_epoch_42.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/200:   0%|▏                                                                  | 4/1151 [00:02<13:42,  1.40it/s]"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "\n",
    "# Check CUDA availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Define constants\n",
    "IMG_SIZE = 512\n",
    "LATENT_DIM = 512\n",
    "NUM_CLASSES = 14\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 200\n",
    "DATASET_DIR = 'MURA-v1.1/train'\n",
    "CHECKPOINT_DIR = 'checkpoints-BAGAN'\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Define class names\n",
    "body_parts = ['XR_WRIST', 'XR_SHOULDER', 'XR_HAND', 'XR_FOREARM', 'XR_FINGER', 'XR_ELBOW', 'XR_HUMERUS']\n",
    "case_types = ['positive', 'negative']\n",
    "class_names = [f\"{bp}_{ct}\" for bp in body_parts for ct in case_types]\n",
    "\n",
    "# Dataset Class\n",
    "class MURADataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            img_path = self.image_paths[idx]\n",
    "            full_path = os.path.join(DATASET_DIR, '..', img_path)\n",
    "            image = Image.open(full_path).convert('L')  # Grayscale for X-rays\n",
    "            \n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            \n",
    "            label = self.labels[idx]\n",
    "            label_onehot = torch.zeros(NUM_CLASSES)\n",
    "            label_onehot[label] = 1.0\n",
    "            \n",
    "            return image, label_onehot\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            placeholder = torch.zeros((1, IMG_SIZE, IMG_SIZE))\n",
    "            return placeholder, torch.zeros(NUM_CLASSES)\n",
    "\n",
    "# Helper Functions for Dataset Processing\n",
    "def get_class_label(image_path):\n",
    "    for bp in body_parts:\n",
    "        if bp in image_path:\n",
    "            case = 'positive' if 'positive' in image_path else 'negative'\n",
    "            class_idx = body_parts.index(bp) * 2 + (0 if case == 'positive' else 1)\n",
    "            return class_idx\n",
    "    return None\n",
    "\n",
    "def scan_dataset(dataset_dir=DATASET_DIR):\n",
    "    print(f\"Scanning MURA dataset directory: {dataset_dir}...\")\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "\n",
    "    for root, dirs, files in os.walk(dataset_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.png'):\n",
    "                full_path = os.path.join(root, file)\n",
    "                relative_path = os.path.relpath(full_path, os.path.join(dataset_dir, '..'))\n",
    "                image_paths.append(relative_path)\n",
    "                labels.append(1 if 'positive' in relative_path else 0)\n",
    "\n",
    "    image_paths = np.array(image_paths)\n",
    "    labels = np.array(labels)\n",
    "    print(f\"Found {len(image_paths)} images with labels\")\n",
    "\n",
    "    print(\"Classifying images...\")\n",
    "    class_labels = []\n",
    "    valid_paths = []\n",
    "\n",
    "    for path in tqdm(image_paths):\n",
    "        label = get_class_label(path)\n",
    "        if label is not None:\n",
    "            class_labels.append(label)\n",
    "            valid_paths.append(path)\n",
    "\n",
    "    valid_paths = np.array(valid_paths)\n",
    "    class_labels = np.array(class_labels)\n",
    "    print(f\"Successfully classified {len(class_labels)} images into classes\")\n",
    "\n",
    "    class_counts = Counter(class_labels)\n",
    "    print(\"Class distribution:\")\n",
    "    for cls in sorted(class_counts.keys()):\n",
    "        print(f\"{class_names[cls]}: {class_counts[cls]} images\")\n",
    "    \n",
    "    return valid_paths, class_labels, class_counts\n",
    "\n",
    "# Autoencoder Architecture (same as BAGAN-GP + WGAN-GP)\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 4, stride=2, padding=1),  # 512 -> 256\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 256 -> 128\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, 4, stride=2, padding=1),  # 128 -> 64\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 512, 4, stride=2, padding=1),  # 64 -> 32\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(512, 1024, 4, stride=2, padding=1),  # 32 -> 16\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.fc = nn.Linear(1024 * 16 * 16, LATENT_DIM)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        latent = self.fc(x)\n",
    "        return latent\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.initial_size = 16\n",
    "        self.fc = nn.Linear(LATENT_DIM + NUM_CLASSES, 1024 * self.initial_size * self.initial_size)\n",
    "        self.bn_initial = nn.BatchNorm2d(1024)\n",
    "        self.deconv = nn.Sequential(\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.ConvTranspose2d(1024, 512, 4, stride=2, padding=1),  # 16 -> 32\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1),  # 32 -> 64\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),  # 64 -> 128\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # 128 -> 256\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.ConvTranspose2d(64, 1, 4, stride=2, padding=1),  # 256 -> 512\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, z, labels):\n",
    "        x = torch.cat([z, labels], dim=1)\n",
    "        x = self.fc(x)\n",
    "        x = x.view(-1, 1024, self.initial_size, self.initial_size)\n",
    "        x = self.bn_initial(x)\n",
    "        x = nn.functional.relu(x, inplace=True)\n",
    "        x = self.deconv(x)\n",
    "        return x\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        self.num_classes = num_classes\n",
    "    \n",
    "    def forward(self, x, labels):\n",
    "        latent = self.encoder(x)\n",
    "        reconstructed = self.decoder(latent, labels)\n",
    "        return reconstructed\n",
    "\n",
    "# Discriminator Network (modified for BGAN, no spectral normalization)\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 4, stride=2, padding=1),  # 512 -> 256\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 256 -> 128\n",
    "            nn.InstanceNorm2d(64, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 128 -> 64\n",
    "            nn.InstanceNorm2d(128, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, 4, stride=2, padding=1),  # 64 -> 32\n",
    "            nn.InstanceNorm2d(256, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 512, 4, stride=2, padding=1),  # 32 -> 16\n",
    "            nn.InstanceNorm2d(512, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.embed_dim = 512\n",
    "        self.class_embedding = nn.Linear(num_classes, self.embed_dim)\n",
    "        ds_size = 16\n",
    "        self.feature_size = 512 * ds_size * ds_size\n",
    "        self.adv_layer = nn.Linear(self.feature_size + self.embed_dim, 1)\n",
    "        self.aux_layer = nn.Linear(self.feature_size + self.embed_dim, num_classes)\n",
    "    \n",
    "    def forward(self, img, labels):\n",
    "        features = self.main(img)\n",
    "        features = self.flatten(features)\n",
    "        label_embedding = self.class_embedding(labels)\n",
    "        combined = torch.cat([features, label_embedding], dim=1)\n",
    "        validity = self.adv_layer(combined)\n",
    "        label_logits = self.aux_layer(combined)\n",
    "        return validity, label_logits\n",
    "\n",
    "# Checkpoint Functions\n",
    "def save_checkpoint(epoch, generator, discriminator, gen_optimizer, disc_optimizer, d_losses, g_losses):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'generator_state_dict': generator.state_dict(),\n",
    "        'discriminator_state_dict': discriminator.state_dict(),\n",
    "        'gen_optimizer_state_dict': gen_optimizer.state_dict(),\n",
    "        'disc_optimizer_state_dict': disc_optimizer.state_dict(),\n",
    "        'd_losses': d_losses,\n",
    "        'g_losses': g_losses\n",
    "    }\n",
    "    checkpoint_path = os.path.join(CHECKPOINT_DIR, f'checkpoint_epoch_{epoch}.pth')\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "def load_checkpoint(checkpoint_path, generator, discriminator, gen_optimizer, disc_optimizer):\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "        discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "        gen_optimizer.load_state_dict(checkpoint['gen_optimizer_state_dict'])\n",
    "        disc_optimizer.load_state_dict(checkpoint['disc_optimizer_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        d_losses = checkpoint['d_losses']\n",
    "        g_losses = checkpoint['g_losses']\n",
    "        print(f\"Loaded checkpoint from {checkpoint_path}, resuming from epoch {epoch + 1}\")\n",
    "        return epoch, d_losses, g_losses\n",
    "    else:\n",
    "        print(f\"No checkpoint found at {checkpoint_path}\")\n",
    "        return 0, [], []\n",
    "\n",
    "# BAGAN with BGAN Training Function\n",
    "def train_bagan_bgan(dataloader, generator, discriminator, num_epochs, device, checkpoint_dir, resume_epoch=12):\n",
    "    # Optimizers\n",
    "    disc_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    gen_optimizer = optim.Adam(generator.parameters(), lr=0.0006, betas=(0.5, 0.999))\n",
    "    \n",
    "    # Loss functions\n",
    "    bce_loss = nn.BCEWithLogitsLoss()\n",
    "    classification_loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Loss tracking\n",
    "    d_losses = []\n",
    "    g_losses = []\n",
    "    \n",
    "    # Resume training if specified\n",
    "    start_epoch = 0\n",
    "    if resume_epoch is not None:\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{resume_epoch}.pth')\n",
    "        start_epoch, d_losses, g_losses = load_checkpoint(\n",
    "            checkpoint_path, generator, discriminator, gen_optimizer, disc_optimizer\n",
    "        )\n",
    "    \n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        generator.train()\n",
    "        discriminator.train()\n",
    "        total_d_loss = 0\n",
    "        total_g_loss = 0\n",
    "        \n",
    "        for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            real_images, labels_onehot = batch\n",
    "            real_images = real_images.to(device)\n",
    "            labels_onehot = labels_onehot.to(device)\n",
    "            labels = torch.argmax(labels_onehot, dim=1)\n",
    "            batch_size = real_images.size(0)\n",
    "            \n",
    "            # Labels for BCE loss\n",
    "            real_label = torch.full((batch_size, 1), 0.9, device=device)  # Label smoothing\n",
    "            fake_label = torch.full((batch_size, 1), 0.1, device=device)\n",
    "            \n",
    "            # Train Discriminator\n",
    "            disc_optimizer.zero_grad()\n",
    "            \n",
    "            # Real images\n",
    "            real_validity, real_label_logits = discriminator(real_images, labels_onehot)\n",
    "            d_loss_real = bce_loss(real_validity, real_label)\n",
    "            class_loss_real = classification_loss(real_label_logits, labels)\n",
    "            \n",
    "            # Fake images\n",
    "            noise = torch.randn(batch_size, LATENT_DIM, device=device)\n",
    "            fake_labels_idx = torch.randint(0, NUM_CLASSES, (batch_size,), device=device)\n",
    "            fake_labels_onehot = torch.zeros(batch_size, NUM_CLASSES, device=device)\n",
    "            fake_labels_onehot.scatter_(1, fake_labels_idx.unsqueeze(1), 1)\n",
    "            \n",
    "            fake_images = generator(noise, fake_labels_onehot)\n",
    "            fake_validity, _ = discriminator(fake_images.detach(), fake_labels_onehot)\n",
    "            d_loss_fake = bce_loss(fake_validity, fake_label)\n",
    "            \n",
    "            # Total discriminator loss\n",
    "            d_loss = d_loss_real + d_loss_fake + class_loss_real\n",
    "            d_loss.backward()\n",
    "            disc_optimizer.step()\n",
    "            \n",
    "            total_d_loss += d_loss.item()\n",
    "            \n",
    "            # Train Generator\n",
    "            gen_optimizer.zero_grad()\n",
    "            \n",
    "            # Generate fake images\n",
    "            noise = torch.randn(batch_size, LATENT_DIM, device=device)\n",
    "            gen_labels_idx = torch.randint(0, NUM_CLASSES, (batch_size,), device=device)\n",
    "            gen_labels_onehot = torch.zeros(batch_size, NUM_CLASSES, device=device)\n",
    "            gen_labels_onehot.scatter_(1, gen_labels_idx.unsqueeze(1), 1)\n",
    "            \n",
    "            fake_images = generator(noise, gen_labels_onehot)\n",
    "            fake_validity, fake_label_logits = discriminator(fake_images, gen_labels_onehot)\n",
    "            \n",
    "            # Boundary-seeking loss\n",
    "            prob = torch.sigmoid(fake_validity)\n",
    "            g_loss_adv = 0.5 * torch.mean(torch.log(prob / (1 - prob + 1e-8))**2)\n",
    "            class_loss_fake = classification_loss(fake_label_logits, gen_labels_idx)    \n",
    "            \n",
    "            # Total generator loss\n",
    "            g_loss = g_loss_adv + 0.5 * class_loss_fake\n",
    "            g_loss.backward()\n",
    "            gen_optimizer.step()\n",
    "            \n",
    "            total_g_loss += g_loss.item()\n",
    "        \n",
    "        # End of epoch\n",
    "        avg_d_loss = total_d_loss / len(dataloader)\n",
    "        avg_g_loss = total_g_loss / len(dataloader)\n",
    "        d_losses.append(avg_d_loss)\n",
    "        g_losses.append(avg_g_loss)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], D Loss: {avg_d_loss:.4f}, G Loss: {avg_g_loss:.4f}\")\n",
    "        \n",
    "        # Save checkpoint and generate samples\n",
    "        if (epoch + 1) % 1 == 0:\n",
    "            save_checkpoint(epoch + 1, generator, discriminator, gen_optimizer, disc_optimizer, d_losses, g_losses)\n",
    "            \n",
    "            # Generate and save sample images\n",
    "            with torch.no_grad():\n",
    "                noise = torch.randn(NUM_CLASSES, LATENT_DIM, device=device)\n",
    "                sample_labels_onehot = torch.eye(NUM_CLASSES, device=device)\n",
    "                fake_images = generator(noise, sample_labels_onehot)\n",
    "                fake_images = (fake_images + 1) / 2  # Scale from [-1,1] to [0,1]\n",
    "                \n",
    "                # Create grid of sample images\n",
    "                fig, axes = plt.subplots(2, 7, figsize=(14, 4))\n",
    "                axes = axes.flatten()\n",
    "                \n",
    "                for i in range(NUM_CLASSES):\n",
    "                    img = fake_images[i].cpu().numpy().squeeze()\n",
    "                    axes[i].imshow(img, cmap='gray')\n",
    "                    axes[i].set_title(class_names[i])\n",
    "                    axes[i].axis('off')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(checkpoint_dir, f'samples_epoch_{epoch+1}.png'))\n",
    "                plt.close()\n",
    "                \n",
    "                # Save individual images\n",
    "                for i in range(NUM_CLASSES):\n",
    "                    img = fake_images[i].cpu().numpy().squeeze()\n",
    "                    plt.imsave(os.path.join(checkpoint_dir, f'epoch_{epoch+1}_class_{i}.png'), img, cmap='gray')\n",
    "\n",
    "# Function to Generate Balanced Dataset\n",
    "def generate_balanced_dataset(generator, class_counts, class_names, num_classes, latent_dim, device, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Calculate target counts for balanced dataset\n",
    "    total_images = sum(class_counts.values())\n",
    "    target_per_body_part = total_images / 7  # Equal distribution across 7 body parts\n",
    "    target_per_class = target_per_body_part / 2  # 50/50 split between positive/negative\n",
    "    \n",
    "    print(f\"Total images: {total_images}\")\n",
    "    print(f\"Target per body part: {target_per_body_part:.0f}\")\n",
    "    print(f\"Target per class: {target_per_class:.0f}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for body_part_idx in range(0, num_classes, 2):\n",
    "            body_part_name = class_names[body_part_idx].split('_')[0]\n",
    "            positive_class = body_part_idx\n",
    "            negative_class = body_part_idx + 1\n",
    "            \n",
    "            current_positive = class_counts.get(positive_class, 0)\n",
    "            current_negative = class_counts.get(negative_class, 0)\n",
    "            \n",
    "            images_needed_positive = max(0, int(target_per_class) - current_positive)\n",
    "            images_needed_negative = max(0, int(target_per_class) - current_negative)\n",
    "            \n",
    "            print(f\"Balancing {body_part_name}:\")\n",
    "            print(f\"  Positive: {current_positive} -> need {images_needed_positive} more\")  \n",
    "            print(f\"  Negative: {current_negative} -> need {images_needed_negative} more\")\n",
    "            \n",
    "            # Create directories\n",
    "            pos_dir = os.path.join(output_dir, class_names[positive_class])\n",
    "            neg_dir = os.path.join(output_dir, class_names[negative_class])\n",
    "            os.makedirs(pos_dir, exist_ok=True)\n",
    "            os.makedirs(neg_dir, exist_ok=True)\n",
    "            \n",
    "            # Generate positive class images\n",
    "            if images_needed_positive > 0:\n",
    "                batch_size = 16\n",
    "                num_batches = (images_needed_positive + batch_size - 1) // batch_size\n",
    "                \n",
    "                for batch in tqdm(range(num_batches), desc=f\"Generating {body_part_name} positive\"):\n",
    "                    current_batch_size = min(batch_size, images_needed_positive - batch * batch_size)\n",
    "                    if current_batch_size <= 0:\n",
    "                        break\n",
    "                        \n",
    "                    noise = torch.randn(current_batch_size, latent_dim, device=device)\n",
    "                    labels_onehot = torch.zeros(current_batch_size, num_classes, device=device)\n",
    "                    labels_onehot[:, positive_class] = 1.0\n",
    "                    \n",
    "                    fake_images = generator(noise, labels_onehot)\n",
    "                    fake_images = (fake_images + 1) / 2  # Scale from [-1,1] to [0,1]\n",
    "                    \n",
    "                    for i in range(current_batch_size):\n",
    "                        img = fake_images[i].cpu().numpy().squeeze()\n",
    "                        img = (img * 255).astype(np.uint8)\n",
    "                        img_pil = Image.fromarray(img, mode='L')\n",
    "                        img_pil.save(os.path.join(pos_dir, f\"synthetic_{batch * batch_size + i}.png\"))\n",
    "            \n",
    "            # Generate negative class images\n",
    "            if images_needed_negative > 0:\n",
    "                batch_size = 16\n",
    "                num_batches = (images_needed_negative + batch_size - 1) // batch_size\n",
    "                \n",
    "                for batch in tqdm(range(num_batches), desc=f\"Generating {body_part_name} negative\"):\n",
    "                    current_batch_size = min(batch_size, images_needed_negative - batch * batch_size)\n",
    "                    if current_batch_size <= 0:\n",
    "                        break\n",
    "                        \n",
    "                    noise = torch.randn(current_batch_size, latent_dim, device=device)\n",
    "                    labels_onehot = torch.zeros(current_batch_size, num_classes, device=device)\n",
    "                    labels_onehot[:, negative_class] = 1.0\n",
    "                    \n",
    "                    fake_images = generator(noise, labels_onehot)\n",
    "                    fake_images = (fake_images + 1) / 2  # Scale from [-1,1] to [0,1]\n",
    "                    \n",
    "                    for i in range(current_batch_size):\n",
    "                        img = fake_images[i].cpu().numpy().squeeze()\n",
    "                        img = (img * 255).astype(np.uint8)\n",
    "                        img_pil = Image.fromarray(img, mode='L')\n",
    "                        img_pil.save(os.path.join(neg_dir, f\"synthetic_{batch * batch_size + i}.png\"))\n",
    "\n",
    "# Load and prepare training data\n",
    "image_paths, labels, class_counts = scan_dataset(DATASET_DIR)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize to [-1, 1] for tanh activation\n",
    "])\n",
    "dataset = MURADataset(image_paths, labels, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "\n",
    "# Load and prepare validation data\n",
    "val_image_paths, val_labels, val_class_counts = scan_dataset('MURA-v1.1/valid')\n",
    "val_dataset = MURADataset(val_image_paths, val_labels, transform=transform)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "# Load pretrained autoencoder\n",
    "autoencoder = Autoencoder(NUM_CLASSES).to(device)\n",
    "checkpoint_path = 'checkpoints-BAGAN-GP-WGAN-GP_Old-2/best_autoencoder.pth'  # Use best checkpoint\n",
    "if os.path.exists(checkpoint_path):\n",
    "    autoencoder.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "    print(f\"Loaded autoencoder checkpoint from {checkpoint_path}\")\n",
    "else:\n",
    "    print(f\"Checkpoint not found at {checkpoint_path}. Please train the autoencoder first.\")\n",
    "    exit()\n",
    "\n",
    "# Extract the pretrained decoder (generator) and initialize discriminator\n",
    "generator = autoencoder.decoder.to(device)\n",
    "discriminator = Discriminator(NUM_CLASSES).to(device)\n",
    "\n",
    "# Train BAGAN with BGAN\n",
    "train_bagan_bgan(dataloader, generator, discriminator, NUM_EPOCHS, device, CHECKPOINT_DIR, resume_epoch=41)\n",
    "\n",
    "# Generate balanced dataset\n",
    "output_dir = 'synthetic_mura'\n",
    "generate_balanced_dataset(generator, class_counts, class_names, NUM_CLASSES, LATENT_DIM, device, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d07dc2-b055-4fa6-820c-54828f08260d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4961d538-9605-4a02-848f-e9e7f205aa4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

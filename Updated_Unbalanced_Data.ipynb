{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90abc336-57c6-47ca-96bb-a09c3773ecea",
   "metadata": {},
   "source": [
    "### GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ebe937d-f814-4239-adc6-dcae3dce6a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set environment variable to avoid memory fragmentation\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(\"MURA-v1.1/checkpoints\", exist_ok=True)\n",
    "os.makedirs(\"MURA-v1.1/synthetic\", exist_ok=True)\n",
    "os.makedirs(\"MURA-v1.1/synthetic/samples\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96fc8ff8-b44e-4d3e-b31d-d9bd64381e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 36808\n",
      "                                          image_path  label     category\n",
      "0  MURA-v1.1/train/XR_SHOULDER/patient00001/study...      1  XR_SHOULDER\n",
      "1  MURA-v1.1/train/XR_SHOULDER/patient00001/study...      1  XR_SHOULDER\n",
      "2  MURA-v1.1/train/XR_SHOULDER/patient00001/study...      1  XR_SHOULDER\n",
      "3  MURA-v1.1/train/XR_SHOULDER/patient00002/study...      1  XR_SHOULDER\n",
      "4  MURA-v1.1/train/XR_SHOULDER/patient00002/study...      1  XR_SHOULDER\n",
      "5  MURA-v1.1/train/XR_SHOULDER/patient00002/study...      1  XR_SHOULDER\n",
      "6  MURA-v1.1/train/XR_SHOULDER/patient00003/study...      1  XR_SHOULDER\n",
      "7  MURA-v1.1/train/XR_SHOULDER/patient00003/study...      1  XR_SHOULDER\n",
      "8  MURA-v1.1/train/XR_SHOULDER/patient00003/study...      1  XR_SHOULDER\n",
      "9  MURA-v1.1/train/XR_SHOULDER/patient00004/study...      1  XR_SHOULDER\n",
      "File exists: MURA-v1.1/train/XR_SHOULDER/patient00001/study1_positive/image1.png\n",
      "File exists: MURA-v1.1/train/XR_SHOULDER/patient00001/study1_positive/image2.png\n",
      "File exists: MURA-v1.1/train/XR_SHOULDER/patient00001/study1_positive/image3.png\n",
      "File exists: MURA-v1.1/train/XR_SHOULDER/patient00002/study1_positive/image1.png\n",
      "File exists: MURA-v1.1/train/XR_SHOULDER/patient00002/study1_positive/image2.png\n",
      "Category counts: {'XR_SHOULDER': 8379, 'XR_HUMERUS': 1272, 'XR_FINGER': 5106, 'XR_ELBOW': 4931, 'XR_WRIST': 9752, 'XR_FOREARM': 1825, 'XR_HAND': 5543}\n",
      "Target count for balancing: 9752\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_8056\\4284465277.py:29: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  dummy = torch.zeros(1, device=device)  # Warm-up\n"
     ]
    }
   ],
   "source": [
    "# Define initial transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "df_train_images = pd.read_csv(\"MURA-v1.1/train_image_paths.csv\", header=None, names=[\"image_path\"])\n",
    "df_train_images[\"label\"] = df_train_images[\"image_path\"].apply(lambda x: 1 if \"positive\" in x else 0)\n",
    "df_train_images[\"category\"] = df_train_images[\"image_path\"].apply(lambda x: x.split('/')[2])\n",
    "\n",
    "# Verify dataset\n",
    "print(f\"Total images: {len(df_train_images)}\")\n",
    "print(df_train_images.head(10))\n",
    "for path in df_train_images[\"image_path\"].head():\n",
    "    print(f\"File {'exists' if os.path.exists(path) else 'not found'}: {path}\")\n",
    "\n",
    "# Compute category counts\n",
    "categories = df_train_images[\"category\"].unique()\n",
    "category_counts = {cat: len(df_train_images[df_train_images[\"category\"] == cat]) for cat in categories}\n",
    "print(\"Category counts:\", category_counts)\n",
    "target_count = max(category_counts.values())\n",
    "print(f\"Target count for balancing: {target_count}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    dummy = torch.zeros(1, device=device)  # Warm-up\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "554b1f12-39d6-44d2-ac58-97439cf70fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category counts: {'XR_SHOULDER': 8379, 'XR_HUMERUS': 1272, 'XR_FINGER': 5106, 'XR_ELBOW': 4931, 'XR_WRIST': 9752, 'XR_FOREARM': 1825, 'XR_HAND': 5543}\n",
      "Target count for balancing: 9752\n"
     ]
    }
   ],
   "source": [
    "# Compute category counts\n",
    "categories = df_train_images[\"category\"].unique()\n",
    "category_counts = {cat: len(df_train_images[df_train_images[\"category\"] == cat]) for cat in categories}\n",
    "print(\"Category counts:\", category_counts)\n",
    "\n",
    "# Target count: maximum number of images in any category\n",
    "target_count = max(category_counts.values())\n",
    "print(f\"Target count for balancing: {target_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cceac9b7-7020-490e-94d7-6bb1b5c07bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    dummy = torch.zeros(1, device=device)  # Warm-up\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1390367c-f4d2-4144-a77a-e3da8a1ae898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "Device name: NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"Device name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a33382c-5bb3-4c25-83b1-61ab6387237c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, scale_factor=2.0, initial_block=False):\n",
    "        super(GeneratorBlock, self).__init__()\n",
    "        self.initial_block = initial_block\n",
    "        if initial_block:\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_channels, out_channels, 4, 1, 0),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.PReLU()\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.Upsample(scale_factor=scale_factor, mode='nearest'),\n",
    "                nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.PReLU()\n",
    "            )\n",
    "        self.to_rgb = nn.Conv2d(out_channels, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        rgb = self.to_rgb(x)\n",
    "        return x, rgb\n",
    "\n",
    "class DiscriminatorBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, initial_block=False):\n",
    "        super(DiscriminatorBlock, self).__init__()\n",
    "        self.initial_block = initial_block\n",
    "        self.from_rgb = nn.Conv2d(1, in_channels, 1)\n",
    "        if initial_block:\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 4, 1, 0),\n",
    "                nn.LeakyReLU(0.2, inplace=True)\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 3, stride=2, padding=1),\n",
    "                nn.LeakyReLU(0.2, inplace=True)\n",
    "            )\n",
    "\n",
    "    def forward(self, x, is_raw_input=False):\n",
    "        if is_raw_input:\n",
    "            x = self.from_rgb(x)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class ProGANGenerator(nn.Module):\n",
    "    def __init__(self, latent_dim, num_classes, resolutions=[4, 8, 16, 32, 64, 112, 224]):\n",
    "        super(ProGANGenerator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.label_emb = nn.Embedding(num_classes, latent_dim)\n",
    "        scale_factors = [resolutions[i+1] / resolutions[i] for i in range(len(resolutions)-1)]\n",
    "        self.blocks = nn.ModuleList([\n",
    "            GeneratorBlock(2 * latent_dim, 512, initial_block=True),  # 4x4\n",
    "            GeneratorBlock(512, 512, scale_factor=scale_factors[0]),  # 8x8\n",
    "            GeneratorBlock(512, 256, scale_factor=scale_factors[1]),  # 16x16\n",
    "            GeneratorBlock(256, 128, scale_factor=scale_factors[2]),  # 32x32\n",
    "            GeneratorBlock(128, 64, scale_factor=scale_factors[3]),   # 64x64\n",
    "            GeneratorBlock(64, 32, scale_factor=scale_factors[4]),    # 112x112\n",
    "            GeneratorBlock(32, 16, scale_factor=scale_factors[5]),    # 224x224\n",
    "        ])\n",
    "        self.current_depth = 0\n",
    "\n",
    "    def forward(self, z, labels, alpha=1.0):\n",
    "        label_embed = self.label_emb(labels)\n",
    "        z = torch.cat([z, label_embed], dim=1)\n",
    "        z = z.view(z.size(0), z.size(1), 1, 1)\n",
    "\n",
    "        x = z\n",
    "        prev_rgb = None\n",
    "        for i in range(self.current_depth + 1):\n",
    "            x, rgb = self.blocks[i](x)\n",
    "            if i == self.current_depth and i > 0:\n",
    "                prev_rgb_up = F.interpolate(prev_rgb, size=rgb.shape[2:], mode='nearest')\n",
    "                rgb = (1 - alpha) * prev_rgb_up + alpha * rgb\n",
    "            prev_rgb = rgb\n",
    "        return rgb\n",
    "\n",
    "class ProGANDiscriminator(nn.Module):\n",
    "    def __init__(self, num_classes, latent_dim=100, resolutions=[4, 8, 16, 32, 64, 112, 224]):\n",
    "        super(ProGANDiscriminator, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.latent_dim = latent_dim\n",
    "        self.label_emb = nn.Embedding(num_classes, latent_dim)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            DiscriminatorBlock(16, 32),    # 224x224 -> 112x112\n",
    "            DiscriminatorBlock(32, 64),    # 112x112 -> 56x56\n",
    "            DiscriminatorBlock(64, 128),   # 56x56 -> 28x28\n",
    "            DiscriminatorBlock(128, 256),  # 28x28 -> 14x14\n",
    "            DiscriminatorBlock(256, 512),  # 14x14 -> 7x7\n",
    "            DiscriminatorBlock(512, 512),  # 8x8 -> 4x4\n",
    "            DiscriminatorBlock(512, 512, initial_block=True),  # 4x4 -> 1x1\n",
    "        ])\n",
    "        self.final_layer = nn.Linear(512 + latent_dim, 1)\n",
    "        self.current_depth = 0\n",
    "        self.resolutions = resolutions\n",
    "\n",
    "    def forward(self, img, labels, alpha=1.0, current_res=None):\n",
    "        if current_res is None:\n",
    "            raise ValueError(\"current_res must be provided\")\n",
    "\n",
    "        x = img\n",
    "        if self.current_depth > 0:\n",
    "            current_block_idx = len(self.blocks) - self.current_depth - 1\n",
    "            if alpha < 1.0:\n",
    "                new_x = self.blocks[current_block_idx](x, is_raw_input=True)\n",
    "                downsampled_x = F.interpolate(x, scale_factor=0.5, mode='nearest')\n",
    "                old_x = self.blocks[current_block_idx + 1].from_rgb(downsampled_x)\n",
    "                x = alpha * new_x + (1 - alpha) * old_x\n",
    "            else:\n",
    "                x = self.blocks[current_block_idx](x, is_raw_input=True)\n",
    "        else:\n",
    "            x = self.blocks[-1](x, is_raw_input=True)\n",
    "\n",
    "        for i in range(current_block_idx + 1, len(self.blocks)):\n",
    "            x = self.blocks[i](x, is_raw_input=False)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        label_embed = self.label_emb(labels).view(labels.size(0), -1)\n",
    "        x = torch.cat([x, label_embed], dim=1)\n",
    "        x = self.final_layer(x)\n",
    "        return x\n",
    "\n",
    "class MURADataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert('L')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "def compute_gradient_penalty(discriminator, real_samples, fake_samples, labels, device, current_res):\n",
    "    alpha = torch.rand(real_samples.size(0), 1, 1, 1, device=device)\n",
    "    interpolates = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)\n",
    "    d_interpolates = discriminator(interpolates, labels, alpha=1.0, current_res=current_res)\n",
    "    fake = torch.ones(real_samples.size(0), 1, device=device, requires_grad=False)\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5883177-7797-44e5-9124-0121ada83f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_progan(generator, discriminator, dataloader, num_epochs_per_resolution, latent_dim, device, \n",
    "                 save_interval=10, save_dir=\"checkpoints\", minority_label=1, category=\"unknown\", start_depth=0):\n",
    "    # Dynamically set hyperparameters based on depth\n",
    "    resolutions = [4, 8, 16, 32, 64, 112, 224]\n",
    "    \n",
    "    for depth in range(start_depth, len(resolutions)):\n",
    "        generator.current_depth = depth\n",
    "        discriminator.current_depth = depth\n",
    "        resolution = resolutions[depth]\n",
    "        print(f\"\\nTraining at resolution {resolution}x{resolution}\")\n",
    "\n",
    "        # Set learning rate and other hyperparameters based on depth\n",
    "        if depth < 3:  # 4x4, 8x8, 16x16\n",
    "            lr = 0.0001\n",
    "            fade_in_steps = 50\n",
    "            n_critic = 5\n",
    "        else:  # 32x32, 64x64, 112x112, 224x224\n",
    "            lr = 0.00005\n",
    "            fade_in_steps = 100\n",
    "            n_critic = 10\n",
    "\n",
    "        g_optimizer = optim.Adam(generator.parameters(), lr=lr, betas=(0, 0.9))\n",
    "        d_optimizer = optim.Adam(discriminator.parameters(), lr=lr, betas=(0, 0.9))\n",
    "        lambda_gp = 10\n",
    "\n",
    "        current_transform = transforms.Compose([\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.Resize((resolution, resolution)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "        ])\n",
    "        dataset = MURADataset(dataloader.dataset.image_paths, dataloader.dataset.labels, transform=current_transform)\n",
    "        current_dataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=0, pin_memory=True)\n",
    "        print(f\"Dataset size: {len(dataset)}, Batches: {len(current_dataloader)}\")\n",
    "\n",
    "        # Fade-in phase with dynamically adjusted steps\n",
    "        for alpha_step, alpha in enumerate(np.linspace(0, 1, fade_in_steps)):\n",
    "            alpha = float(alpha)\n",
    "            print(f\"Fade-in phase, alpha={alpha:.4f}\")\n",
    "            running_d_loss = 0.0\n",
    "            running_g_loss = 0.0\n",
    "            for i, (real_imgs, labels) in enumerate(tqdm(current_dataloader, desc=f\"Alpha {alpha:.4f}\")):\n",
    "                real_imgs = real_imgs.to(device, non_blocking=True)\n",
    "                labels = labels.to(device, non_blocking=True)\n",
    "                batch_size = real_imgs.size(0)\n",
    "\n",
    "                # Discriminator update\n",
    "                for _ in range(n_critic):\n",
    "                    d_optimizer.zero_grad()\n",
    "                    z = torch.randn(batch_size, latent_dim, device=device)\n",
    "                    fake_labels = torch.full((batch_size,), minority_label, dtype=torch.long, device=device)\n",
    "                    fake_imgs = generator(z, fake_labels, alpha=alpha)\n",
    "\n",
    "                    real_validity = discriminator(real_imgs, labels, alpha=alpha, current_res=resolution)\n",
    "                    fake_validity = discriminator(fake_imgs.detach(), fake_labels, alpha=alpha, current_res=resolution)\n",
    "                    gradient_penalty = compute_gradient_penalty(discriminator, real_imgs, fake_imgs.detach(), \n",
    "                                                              labels, device, current_res=resolution)\n",
    "                    d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gradient_penalty\n",
    "                    d_loss.backward()\n",
    "                    d_optimizer.step()\n",
    "                    running_d_loss += d_loss.item()\n",
    "\n",
    "                # Generator update\n",
    "                g_optimizer.zero_grad()\n",
    "                fake_imgs = generator(z, fake_labels, alpha=alpha)\n",
    "                fake_validity = discriminator(fake_imgs, fake_labels, alpha=alpha, current_res=resolution)\n",
    "                g_loss = -torch.mean(fake_validity)\n",
    "                g_loss.backward()\n",
    "                g_optimizer.step()\n",
    "                running_g_loss += g_loss.item()\n",
    "\n",
    "            avg_d_loss = running_d_loss / (len(current_dataloader) * n_critic)\n",
    "            avg_g_loss = running_g_loss / len(current_dataloader)\n",
    "            print(f\"Alpha {alpha:.4f}, Avg D Loss: {avg_d_loss:.4f}, Avg G Loss: {avg_g_loss:.4f}\")\n",
    "\n",
    "            # Save checkpoint\n",
    "            if alpha_step % save_interval == 0:\n",
    "                torch.save({\n",
    "                    'depth': depth,\n",
    "                    'alpha': alpha,\n",
    "                    'generator_state_dict': generator.state_dict(),\n",
    "                    'discriminator_state_dict': discriminator.state_dict(),\n",
    "                    'g_optimizer_state_dict': g_optimizer.state_dict(),\n",
    "                    'd_optimizer_state_dict': d_optimizer.state_dict(),\n",
    "                }, os.path.join(save_dir, f\"progan_{category}_{'positive' if minority_label == 1 else 'negative'}_depth{depth}_alpha{alpha:.4f}.pt\"))\n",
    "\n",
    "        # Stabilization phase\n",
    "        for epoch in range(num_epochs_per_resolution - fade_in_steps):\n",
    "            running_d_loss = 0.0\n",
    "            running_g_loss = 0.0\n",
    "            for i, (real_imgs, labels) in enumerate(tqdm(current_dataloader, desc=f\"Epoch {epoch+1}\")):\n",
    "                real_imgs = real_imgs.to(device, non_blocking=True)\n",
    "                labels = labels.to(device, non_blocking=True)\n",
    "                batch_size = real_imgs.size(0)\n",
    "\n",
    "                # Discriminator update\n",
    "                for _ in range(n_critic):\n",
    "                    d_optimizer.zero_grad()\n",
    "                    z = torch.randn(batch_size, latent_dim, device=device)\n",
    "                    fake_labels = torch.full((batch_size,), minority_label, dtype=torch.long, device=device)\n",
    "                    fake_imgs = generator(z, fake_labels, alpha=1.0)\n",
    "\n",
    "                    real_validity = discriminator(real_imgs, labels, alpha=1.0, current_res=resolution)\n",
    "                    fake_validity = discriminator(fake_imgs.detach(), fake_labels, alpha=1.0, current_res=resolution)\n",
    "                    gradient_penalty = compute_gradient_penalty(discriminator, real_imgs, fake_imgs.detach(), \n",
    "                                                              labels, device, current_res=resolution)\n",
    "                    d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gradient_penalty\n",
    "                    d_loss.backward()\n",
    "                    d_optimizer.step()\n",
    "                    running_d_loss += d_loss.item()\n",
    "\n",
    "                # Generator update\n",
    "                g_optimizer.zero_grad()\n",
    "                fake_imgs = generator(z, fake_labels, alpha=1.0)\n",
    "                fake_validity = discriminator(fake_imgs, fake_labels, alpha=1.0, current_res=resolution)\n",
    "                g_loss = -torch.mean(fake_validity)\n",
    "                g_loss.backward()\n",
    "                g_optimizer.step()\n",
    "                running_g_loss += g_loss.item()\n",
    "\n",
    "            avg_d_loss = running_d_loss / (len(current_dataloader) * n_critic)\n",
    "            avg_g_loss = running_g_loss / len(current_dataloader)\n",
    "            print(f\"Depth {depth}, Epoch {epoch+1}/{num_epochs_per_resolution - fade_in_steps}, Avg D Loss: {avg_d_loss:.4f}, Avg G Loss: {avg_g_loss:.4f}\")\n",
    "\n",
    "            if (epoch + 1) % save_interval == 0:\n",
    "                with torch.no_grad():\n",
    "                    sample_z = torch.randn(5, latent_dim, device=device)\n",
    "                    sample_labels = torch.full((5,), minority_label, dtype=torch.long, device=device)\n",
    "                    sample_imgs = generator(sample_z, sample_labels, alpha=1.0)\n",
    "                    sample_imgs = (sample_imgs * 0.5 + 0.5) * 255\n",
    "                    sample_imgs = sample_imgs.cpu().numpy().astype(np.uint8)\n",
    "                    for j in range(5):\n",
    "                        Image.fromarray(sample_imgs[j, 0], mode='L').save(\n",
    "                            f\"MURA-v1.1/synthetic/samples/{category}_{'positive' if minority_label == 1 else 'negative'}_depth{depth}_epoch{epoch+1}_{j}.png\"\n",
    "                        )\n",
    "\n",
    "        torch.save({\n",
    "            'depth': depth,\n",
    "            'generator_state_dict': generator.state_dict(),\n",
    "            'discriminator_state_dict': discriminator.state_dict(),\n",
    "            'g_optimizer_state_dict': g_optimizer.state_dict(),\n",
    "            'd_optimizer_state_dict': d_optimizer.state_dict(),\n",
    "        }, os.path.join(save_dir, f\"progan_{category}_{'positive' if minority_label == 1 else 'negative'}_depth{depth}.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ee2fd0d-1f1c-4836-8990-1b946d9752c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Balancing body part XR_SHOULDER to target count 9752...\n",
      "Current Positive: 4168, Negative: 4211\n",
      "Generating 708 synthetic positive images for XR_SHOULDER...\n",
      "Resuming training for XR_SHOULDER (positive) from depth 3\n",
      "\n",
      "Training at resolution 32x32\n",
      "Dataset size: 4168, Batches: 261\n",
      "Fade-in phase, alpha=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.0000:   0%|                                                                            | 0/261 [00:00<?, ?it/s]C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\cuda\\CublasHandlePool.cpp:135.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "Alpha 0.0000: 100%|██████████████████████████████████████████████████████████████████| 261/261 [02:17<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.0000, Avg D Loss: -23.8654, Avg G Loss: 2406.3658\n",
      "Fade-in phase, alpha=0.0101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.0101: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:20<00:00,  3.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.0101, Avg D Loss: -497.8861, Avg G Loss: -505.6652\n",
      "Fade-in phase, alpha=0.0202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.0202: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:20<00:00,  3.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.0202, Avg D Loss: -1194.9336, Avg G Loss: -704.5119\n",
      "Fade-in phase, alpha=0.0303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.0303: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:20<00:00,  3.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.0303, Avg D Loss: -2340.2898, Avg G Loss: 12330.0609\n",
      "Fade-in phase, alpha=0.0404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.0404: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:20<00:00,  3.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.0404, Avg D Loss: -2799.7801, Avg G Loss: 30395.2043\n",
      "Fade-in phase, alpha=0.0505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.0505: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:21<00:00,  3.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.0505, Avg D Loss: -1766.2016, Avg G Loss: 9989.8116\n",
      "Fade-in phase, alpha=0.0606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.0606: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:20<00:00,  3.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.0606, Avg D Loss: -538.7872, Avg G Loss: -11.0271\n",
      "Fade-in phase, alpha=0.0707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.0707: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:20<00:00,  3.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.0707, Avg D Loss: -6.1127, Avg G Loss: -13186.0964\n",
      "Fade-in phase, alpha=0.0808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.0808: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:20<00:00,  3.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.0808, Avg D Loss: 390.0960, Avg G Loss: -31472.5776\n",
      "Fade-in phase, alpha=0.0909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.0909: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:20<00:00,  3.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.0909, Avg D Loss: 1707.0580, Avg G Loss: -5160.1198\n",
      "Fade-in phase, alpha=0.1010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.1010: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:20<00:00,  3.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.1010, Avg D Loss: 722.7686, Avg G Loss: 4039.7216\n",
      "Fade-in phase, alpha=0.1111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.1111: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:20<00:00,  3.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.1111, Avg D Loss: -382.0679, Avg G Loss: -9129.7762\n",
      "Fade-in phase, alpha=0.1212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.1212: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:20<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.1212, Avg D Loss: -1206.5261, Avg G Loss: -7346.7566\n",
      "Fade-in phase, alpha=0.1313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.1313: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:20<00:00,  3.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.1313, Avg D Loss: -1415.0479, Avg G Loss: -7995.2602\n",
      "Fade-in phase, alpha=0.1414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.1414: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:20<00:00,  3.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.1414, Avg D Loss: -758.1689, Avg G Loss: -19890.4647\n",
      "Fade-in phase, alpha=0.1515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.1515: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:20<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.1515, Avg D Loss: -679.8765, Avg G Loss: 10839.6419\n",
      "Fade-in phase, alpha=0.1616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.1616: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:20<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.1616, Avg D Loss: 234.0592, Avg G Loss: 1946.7881\n",
      "Fade-in phase, alpha=0.1717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.1717: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:20<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.1717, Avg D Loss: 150.2831, Avg G Loss: -4568.1535\n",
      "Fade-in phase, alpha=0.1818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.1818: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:20<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.1818, Avg D Loss: 489.9748, Avg G Loss: -3117.2110\n",
      "Fade-in phase, alpha=0.1919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.1919: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:20<00:00,  3.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.1919, Avg D Loss: -1304.6859, Avg G Loss: -11012.7098\n",
      "Fade-in phase, alpha=0.2020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.2020: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:20<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.2020, Avg D Loss: -1345.0904, Avg G Loss: -9960.9634\n",
      "Fade-in phase, alpha=0.2121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.2121: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:19<00:00,  3.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.2121, Avg D Loss: -955.0456, Avg G Loss: -5005.1374\n",
      "Fade-in phase, alpha=0.2222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.2222: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:20<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.2222, Avg D Loss: -682.5065, Avg G Loss: -10251.2588\n",
      "Fade-in phase, alpha=0.2323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.2323: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:20<00:00,  3.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.2323, Avg D Loss: -1957.4077, Avg G Loss: -21634.8861\n",
      "Fade-in phase, alpha=0.2424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.2424: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:20<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.2424, Avg D Loss: -2410.9111, Avg G Loss: -10744.2514\n",
      "Fade-in phase, alpha=0.2525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.2525: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:19<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.2525, Avg D Loss: -1861.8455, Avg G Loss: -10889.1515\n",
      "Fade-in phase, alpha=0.2626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.2626: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:20<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.2626, Avg D Loss: -3256.0978, Avg G Loss: -4507.7050\n",
      "Fade-in phase, alpha=0.2727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.2727: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:20<00:00,  3.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.2727, Avg D Loss: -2667.3603, Avg G Loss: 17261.9497\n",
      "Fade-in phase, alpha=0.2828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.2828: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:21<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.2828, Avg D Loss: -4288.8154, Avg G Loss: 7652.4371\n",
      "Fade-in phase, alpha=0.2929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.2929: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:20<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.2929, Avg D Loss: -5028.3060, Avg G Loss: 14400.0480\n",
      "Fade-in phase, alpha=0.3030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.3030: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:20<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.3030, Avg D Loss: -5354.9326, Avg G Loss: 21756.7783\n",
      "Fade-in phase, alpha=0.3131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.3131: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:20<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.3131, Avg D Loss: -5616.0365, Avg G Loss: 18194.1960\n",
      "Fade-in phase, alpha=0.3232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.3232: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:20<00:00,  3.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.3232, Avg D Loss: -6060.8776, Avg G Loss: 14051.7336\n",
      "Fade-in phase, alpha=0.3333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.3333: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:20<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.3333, Avg D Loss: -5709.3856, Avg G Loss: 17262.8763\n",
      "Fade-in phase, alpha=0.3434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.3434: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:20<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.3434, Avg D Loss: -6966.4034, Avg G Loss: 32082.0866\n",
      "Fade-in phase, alpha=0.3535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.3535: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:20<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.3535, Avg D Loss: -9229.4275, Avg G Loss: 27457.0577\n",
      "Fade-in phase, alpha=0.3636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.3636: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:20<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.3636, Avg D Loss: -9640.6923, Avg G Loss: 13017.6106\n",
      "Fade-in phase, alpha=0.3737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.3737: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:20<00:00,  3.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.3737, Avg D Loss: -3621.4622, Avg G Loss: -3018.0652\n",
      "Fade-in phase, alpha=0.3838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.3838: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:20<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.3838, Avg D Loss: -3326.8937, Avg G Loss: 17476.9214\n",
      "Fade-in phase, alpha=0.3939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.3939: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:20<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.3939, Avg D Loss: -3399.7061, Avg G Loss: 28414.9603\n",
      "Fade-in phase, alpha=0.4040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.4040: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:20<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.4040, Avg D Loss: -3705.7930, Avg G Loss: 30531.2496\n",
      "Fade-in phase, alpha=0.4141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.4141: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:20<00:00,  3.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.4141, Avg D Loss: -1003.8509, Avg G Loss: 18297.4447\n",
      "Fade-in phase, alpha=0.4242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.4242: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:20<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.4242, Avg D Loss: -3270.5927, Avg G Loss: 50443.2188\n",
      "Fade-in phase, alpha=0.4343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.4343: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:20<00:00,  3.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.4343, Avg D Loss: -753.3604, Avg G Loss: 31942.3985\n",
      "Fade-in phase, alpha=0.4444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.4444: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:20<00:00,  3.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.4444, Avg D Loss: 6774.6008, Avg G Loss: 26958.5118\n",
      "Fade-in phase, alpha=0.4545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.4545: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:21<00:00,  3.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.4545, Avg D Loss: 5011.7360, Avg G Loss: 60198.1558\n",
      "Fade-in phase, alpha=0.4646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.4646: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:20<00:00,  3.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.4646, Avg D Loss: 3355.9344, Avg G Loss: 50288.2621\n",
      "Fade-in phase, alpha=0.4747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.4747: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:20<00:00,  3.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.4747, Avg D Loss: 665.7271, Avg G Loss: 50348.8392\n",
      "Fade-in phase, alpha=0.4848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.4848: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:20<00:00,  3.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.4848, Avg D Loss: 4587.1067, Avg G Loss: 38669.4820\n",
      "Fade-in phase, alpha=0.4949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.4949: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:17<00:00,  3.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.4949, Avg D Loss: 4051.2152, Avg G Loss: 28107.5005\n",
      "Fade-in phase, alpha=0.5051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.5051: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:17<00:00,  3.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.5051, Avg D Loss: 4661.7464, Avg G Loss: 25682.4784\n",
      "Fade-in phase, alpha=0.5152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.5152: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:17<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.5152, Avg D Loss: 8902.9673, Avg G Loss: 16308.8151\n",
      "Fade-in phase, alpha=0.5253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.5253: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:17<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.5253, Avg D Loss: 9105.5305, Avg G Loss: -1897.0887\n",
      "Fade-in phase, alpha=0.5354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.5354: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:17<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.5354, Avg D Loss: 7013.1279, Avg G Loss: 15916.3803\n",
      "Fade-in phase, alpha=0.5455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.5455: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:17<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.5455, Avg D Loss: 5091.0304, Avg G Loss: 28108.7663\n",
      "Fade-in phase, alpha=0.5556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.5556: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:17<00:00,  3.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.5556, Avg D Loss: 5238.0666, Avg G Loss: -36111.7696\n",
      "Fade-in phase, alpha=0.5657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.5657: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:17<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.5657, Avg D Loss: 7556.1629, Avg G Loss: 42336.4614\n",
      "Fade-in phase, alpha=0.5758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.5758: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:17<00:00,  3.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.5758, Avg D Loss: 6333.7911, Avg G Loss: -1262.2097\n",
      "Fade-in phase, alpha=0.5859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.5859: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:17<00:00,  3.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.5859, Avg D Loss: 6936.2875, Avg G Loss: 10941.0361\n",
      "Fade-in phase, alpha=0.5960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.5960: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:17<00:00,  3.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.5960, Avg D Loss: 6518.6997, Avg G Loss: -21050.0833\n",
      "Fade-in phase, alpha=0.6061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.6061: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:17<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.6061, Avg D Loss: 7371.2436, Avg G Loss: 19104.6760\n",
      "Fade-in phase, alpha=0.6162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.6162: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:17<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.6162, Avg D Loss: 8953.5805, Avg G Loss: -73358.8367\n",
      "Fade-in phase, alpha=0.6263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.6263: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:17<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.6263, Avg D Loss: 5312.9312, Avg G Loss: -1860.9376\n",
      "Fade-in phase, alpha=0.6364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.6364: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:17<00:00,  3.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.6364, Avg D Loss: 6947.0841, Avg G Loss: -11713.0378\n",
      "Fade-in phase, alpha=0.6465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.6465: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:17<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.6465, Avg D Loss: 8703.0777, Avg G Loss: 37790.2045\n",
      "Fade-in phase, alpha=0.6566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.6566: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:17<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.6566, Avg D Loss: 6984.1266, Avg G Loss: 17587.5968\n",
      "Fade-in phase, alpha=0.6667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.6667: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:17<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.6667, Avg D Loss: 8238.2782, Avg G Loss: -10361.0412\n",
      "Fade-in phase, alpha=0.6768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.6768: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:17<00:00,  3.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.6768, Avg D Loss: 26159.2243, Avg G Loss: 1776.5195\n",
      "Fade-in phase, alpha=0.6869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.6869: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:17<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.6869, Avg D Loss: 21185.3032, Avg G Loss: -66987.3914\n",
      "Fade-in phase, alpha=0.6970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.6970: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:17<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.6970, Avg D Loss: 56216.4120, Avg G Loss: 100372.5667\n",
      "Fade-in phase, alpha=0.7071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.7071: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:17<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.7071, Avg D Loss: 19268.5636, Avg G Loss: 139255.4553\n",
      "Fade-in phase, alpha=0.7172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.7172: 100%|██████████████████████████████████████████████████████████████████| 261/261 [01:17<00:00,  3.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.7172, Avg D Loss: 17833.5435, Avg G Loss: 53773.2284\n",
      "Fade-in phase, alpha=0.7273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alpha 0.7273:  67%|█████████████████████████████████████████▌                    | 175/261 [2:22:38<1:10:06, 48.91s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 69\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo checkpoint found for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpositive\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mminority_label\u001b[38;5;250m \u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnegative\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m), starting from scratch.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m start_depth \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m([\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m112\u001b[39m, \u001b[38;5;241m224\u001b[39m]):\n\u001b[1;32m---> 69\u001b[0m     \u001b[43mtrain_progan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogan_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs_per_resolution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m                 \u001b[49m\u001b[43msave_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mminority_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mminority_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_depth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m synthetic_images \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     74\u001b[0m synthetic_labels \u001b[38;5;241m=\u001b[39m [minority_label] \u001b[38;5;241m*\u001b[39m num_synthetic\n",
      "Cell \u001b[1;32mIn[7], line 61\u001b[0m, in \u001b[0;36mtrain_progan\u001b[1;34m(generator, discriminator, dataloader, num_epochs_per_resolution, latent_dim, device, save_interval, save_dir, minority_label, category, start_depth)\u001b[0m\n\u001b[0;32m     59\u001b[0m     d_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     60\u001b[0m     d_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 61\u001b[0m     running_d_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43md_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Generator update\u001b[39;00m\n\u001b[0;32m     64\u001b[0m g_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "latent_dim = 100\n",
    "num_epochs_per_resolution = 100  # Increased for better convergence\n",
    "batch_size = 16\n",
    "save_interval = 10\n",
    "checkpoint_dir = \"MURA-v1.1/checkpoints\"\n",
    "generation_batch_size = 64\n",
    "\n",
    "# Dictionary to store synthetic image paths\n",
    "synthetic_data = []\n",
    "\n",
    "# For each category, balance to target count with equal positive/negative\n",
    "for category in categories:\n",
    "    print(f\"\\nBalancing body part {category} to target count {target_count}...\")\n",
    "    \n",
    "    category_data = df_train_images[df_train_images[\"category\"] == category]\n",
    "    positive_count = len(category_data[category_data[\"label\"] == 1])\n",
    "    negative_count = len(category_data[category_data[\"label\"] == 0])\n",
    "    print(f\"Current Positive: {positive_count}, Negative: {negative_count}\")\n",
    "\n",
    "    target_positive = target_negative = target_count // 2\n",
    "    current_total = positive_count + negative_count\n",
    "\n",
    "    if current_total >= target_count:\n",
    "        if positive_count > target_positive:\n",
    "            num_synthetic_positive = 0\n",
    "            num_synthetic_negative = target_negative - negative_count\n",
    "        elif negative_count > target_negative:\n",
    "            num_synthetic_positive = target_positive - positive_count\n",
    "            num_synthetic_negative = 0\n",
    "        else:\n",
    "            num_synthetic_positive = target_positive - positive_count\n",
    "            num_synthetic_negative = target_negative - negative_count\n",
    "    else:\n",
    "        num_synthetic_positive = target_positive - positive_count\n",
    "        num_synthetic_negative = target_negative - negative_count\n",
    "\n",
    "    for minority_label, num_synthetic in [(1, num_synthetic_positive), (0, num_synthetic_negative)]:\n",
    "        if num_synthetic > 0:\n",
    "            print(f\"Generating {num_synthetic} synthetic {'positive' if minority_label == 1 else 'negative'} images for {category}...\")\n",
    "            minority_images = category_data[category_data[\"label\"] == minority_label][\"image_path\"].tolist()\n",
    "            minority_labels = [minority_label] * len(minority_images)\n",
    "            progan_dataset = MURADataset(minority_images, minority_labels, transform=transform)\n",
    "            progan_loader = DataLoader(progan_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "        \n",
    "            generator = ProGANGenerator(latent_dim, num_classes=2).to(device)\n",
    "            discriminator = ProGANDiscriminator(num_classes=2, latent_dim=latent_dim).to(device)\n",
    "\n",
    "            # Define optimizers before loading checkpoint\n",
    "            import torch.optim as optim\n",
    "            g_optimizer = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "            d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "            # Dynamic checkpoint path\n",
    "            checkpoint_path = r\"D:\\Sem 6 project\\MURA-v1.1\\checkpoints\\progan_XR_SHOULDER_positive_depth2.pt\"\n",
    "            start_depth = 0\n",
    "            if os.path.exists(checkpoint_path):\n",
    "                checkpoint = torch.load(checkpoint_path, weights_only=False)  # Adjust weights_only as needed\n",
    "                generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "                discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "                g_optimizer.load_state_dict(checkpoint['g_optimizer_state_dict'])\n",
    "                d_optimizer.load_state_dict(checkpoint['d_optimizer_state_dict'])\n",
    "                start_depth = checkpoint['depth'] + 1  # Resume from next depth\n",
    "                print(f\"Resuming training for {category} ({'positive' if minority_label == 1 else 'negative'}) from depth {start_depth}\")\n",
    "            else:\n",
    "                print(f\"No checkpoint found for {category} ({'positive' if minority_label == 1 else 'negative'}), starting from scratch.\")\n",
    "\n",
    "            if start_depth < len([4, 8, 16, 32, 64, 112, 224]):\n",
    "                train_progan(generator, discriminator, progan_loader, num_epochs_per_resolution, latent_dim, device,\n",
    "                             save_interval=save_interval, save_dir=checkpoint_dir, \n",
    "                             minority_label=minority_label, category=category, start_depth=start_depth)\n",
    "\n",
    "            synthetic_images = []\n",
    "            synthetic_labels = [minority_label] * num_synthetic\n",
    "            for i in range(0, num_synthetic, generation_batch_size):\n",
    "                batch_size = min(generation_batch_size, num_synthetic - i)\n",
    "                batch_labels = torch.tensor(synthetic_labels[i:i + batch_size], dtype=torch.long, device=device)\n",
    "                with torch.no_grad():\n",
    "                    z = torch.randn(batch_size, latent_dim, device=device)\n",
    "                    batch_images = generator(z, batch_labels, alpha=1.0)\n",
    "                    synthetic_images.append(batch_images.cpu())\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "\n",
    "            synthetic_images = torch.cat(synthetic_images, dim=0)\n",
    "            synthetic_dir = f\"MURA-v1.1/synthetic/{category}/{'positive' if minority_label == 1 else 'negative'}\"\n",
    "            os.makedirs(synthetic_dir, exist_ok=True)\n",
    "            for i, img in enumerate(synthetic_images):\n",
    "                img = img.squeeze().numpy()\n",
    "                img = (img * 0.5) + 0.5\n",
    "                img = (img * 255).astype(np.uint8)\n",
    "                img_path = os.path.join(synthetic_dir, f\"synthetic_{i}.png\")\n",
    "                Image.fromarray(img, mode='L').save(img_path)\n",
    "                synthetic_data.append([img_path, minority_label, category])\n",
    "\n",
    "            del generator, discriminator, progan_dataset, progan_loader, synthetic_images\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "print(\"Positive/Negative balancing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e367f0-ce34-4d5a-8510-8d79008f2ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target number of images per category (match the largest category)\n",
    "target_count = max(category_counts.values())\n",
    "print(f\"Target number of images per category: {target_count}\")\n",
    "\n",
    "# For each category, balance the total number of images\n",
    "for category in categories:\n",
    "    print(f\"\\nBalancing body part {category}...\")\n",
    "    \n",
    "    current_count = category_counts[category]\n",
    "    num_synthetic_total = target_count - current_count\n",
    "\n",
    "    if num_synthetic_cing needed for {category}\")\n",
    "        continue\n",
    "\n",
    "    # Get current positive/negative ratio\n",
    "    category_data = df_train_images[df_train_images[\"category\"] == category]\n",
    "    positive_count = len(category_data[category_data[\"label\"] == 1])\n",
    "    negative_count = len(category_data[category_dtotal <= 0:\n",
    "        print(f\"No balanata[\"label\"] == 0])\n",
    "    total = positive_count + negative_count\n",
    "    positive_ratio = positive_count / total\n",
    "\n",
    "    # Calculate how many positive/negative synthetic images to generate\n",
    "    num_positive_synthetic = int(num_synthetic_total * positive_ratio)\n",
    "    num_negative_synthetic = num_synthetic_total - num_positive_synthetic\n",
    "\n",
    "    # Generate synthetic positive images\n",
    "    if num_positive_synthetic > 0:\n",
    "        positive_images = category_data[category_data[\"label\"] == 1][0].tolist()\n",
    "        positive_labels = [1] * len(positive_images)\n",
    "        bagan_dataset = MURADataset(positive_images, positive_labels, transform=transform)\n",
    "        bagan_loader = DataLoader(bagan_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "        generator = Generator(latent_dim, num_classes=2).to(device)\n",
    "        discriminator = Discriminator(num_classes=2).to(device)\n",
    "        train_bagan(generator, discriminator, bagan_loader, num_epochs, latent_dim, device)\n",
    "\n",
    "        synthetic_positive = generate_synthetic_images(generator, num_positive_synthetic, [1] * num_positive_synthetic, latent_dim, device)\n",
    "        synthetic_dir = f\"MURA-v1.1/synthetic/{category}/positive\"\n",
    "        os.makedirs(synthetic_dir, exist_ok=True)\n",
    "        for i, img in enumerate(synthetic_positive):\n",
    "            img = img.cpu().permute(1, 2, 0).numpy()\n",
    "            img = (img * 0.5) + 0.5\n",
    "            img = (img * 255).astype(np.uint8)\n",
    "            img_path = os.path.join(synthetic_dir, f\"synthetic_{len(positive_images) + i}.png\")\n",
    "            Image.fromarray(img).save(img_path)\n",
    "            synthetic_data.append([img_path, 1, category])\n",
    "\n",
    "    # Generate synthetic negative images\n",
    "    if num_negative_synthetic > 0:\n",
    "        negative_images = category_data[category_data[\"label\"] == 0][0].tolist()\n",
    "        negative_labels = [0] * len(negative_images)\n",
    "        bagan_dataset = MURADataset(negative_images, negative_labels, transform=transform)\n",
    "        bagan_loader = DataLoader(bagan_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "        generator = Generator(latent_dim, num_classes=2).to(device)\n",
    "        discriminator = Discriminator(num_classes=2).to(device)\n",
    "        train_bagan(generator, discriminator, bagan_loader, num_epochs, latent_dim, device)\n",
    "\n",
    "        synthetic_negative = generate_synthetic_images(generator, num_negative_synthetic, [0] * num_negative_synthetic, latent_dim, device)\n",
    "        synthetic_dir = f\"MURA-v1.1/synthetic/{category}/negative\"\n",
    "        os.makedirs(synthetic_dir, exist_ok=True)\n",
    "        for i, img in enumerate(synthetic_negative):\n",
    "            img = img.cpu().permute(1, 2, 0).numpy()\n",
    "            img = (img * 0.5) + 0.5\n",
    "            img = (img * 255).astype(np.uint8)\n",
    "            img_path = os.path.join(synthetic_dir, f\"synthetic_{len(negative_images) + i}.png\")\n",
    "            Image.fromarray(img).save(img_path)\n",
    "            synthetic_data.append([img_path, 0, category])\n",
    "\n",
    "print(\"Body part balancing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5183905-caae-46ec-ada1-8877204b0daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Generate synthetic images\n",
    "    synthetic_labels = [minority_label] * num_synthetic\n",
    "    synthetic_images = generate_synthetic_images(generator, num_synthetic, synthetic_labels, latent_dim, device)\n",
    "\n",
    "    # Save synthetic images\n",
    "    synthetic_dir = f\"MURA-v1.1/synthetic/{category}/{'positive' if minority_label == 1 else 'negative'}\"\n",
    "    os.makedirs(synthetic_dir, exist_ok=True)\n",
    "    for i, img in enumerate(synthetic_images):\n",
    "        img = img.cpu().squeeze().numpy()  # Remove channel dimension for grayscale\n",
    "        img = (img * 0.5) + 0.5  # Denormalize\n",
    "        img = (img * 255).astype(np.uint8)\n",
    "        img_path = os.path.join(synthetic_dir, f\"synthetic_{i}.png\")\n",
    "        Image.fromarray(img, mode='L').save(img_path)  # Save as grayscale\n",
    "        synthetic_data.append([img_path, minority_label, category])\n",
    "\n",
    "print(\"Positive/Negative balancing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01718183-1f0d-4e87-968d-52154b67ec6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65491f8-8238-4d0e-b90e-d1d8bc65ac40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d818c5-55f7-40ee-870f-c056effd8d04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd01784d-f193-4aac-a122-8645eb5c5e89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57485344-5f81-42d5-87f7-02bc0972aee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set environment variable to avoid memory fragmentation\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Create directories for checkpoints and synthetic images\n",
    "os.makedirs(\"MURA-v1.1/checkpoints\", exist_ok=True)\n",
    "os.makedirs(\"MURA-v1.1/synthetic\", exist_ok=True)\n",
    "os.makedirs(\"MURA-v1.1/synthetic/samples\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f9af1a-792b-4304-a2ec-dfed8bf9c052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Scale to [-1, 1]\n",
    "])\n",
    "\n",
    "# Load the dataset (assuming train.csv is provided)\n",
    "df_train_images = pd.read_csv(\"MURA-v1.1/train_image_paths.csv\", header=None, names=[\"image_path\"])\n",
    "df_train_images[\"label\"] = df_train_images[\"image_path\"].apply(lambda x: 1 if \"positive\" in x else 0)\n",
    "df_train_images[\"category\"] = df_train_images[\"image_path\"].apply(lambda x: x.split('/')[2])\n",
    "\n",
    "# Verify the dataset\n",
    "print(f\"Total images: {len(df_train_images)}\")\n",
    "print(df_train_images.head(10))\n",
    "\n",
    "# Verify file existence for the first few paths\n",
    "for path in df_train_images[\"image_path\"].head():\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"File not found: {path}\")\n",
    "    else:\n",
    "        print(f\"File exists: {path}\")\n",
    "\n",
    "# Compute category counts\n",
    "categories = df_train_images[\"category\"].unique()\n",
    "category_counts = {cat: len(df_train_images[df_train_images[\"category\"] == cat]) for cat in categories}\n",
    "print(\"Category counts:\", category_counts)\n",
    "\n",
    "# Target count: maximum number of images in any category\n",
    "target_count = max(category_counts.values())\n",
    "print(f\"Target count for balancing: {target_count}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    dummy = torch.zeros(1, device=device)  # Warm-up\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6caaebe-8c0c-4d27-a89a-27dea1951730",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, scale_factor=2.0, initial_block=False):\n",
    "        super(GeneratorBlock, self).__init__()\n",
    "        self.initial_block = initial_block\n",
    "        if initial_block:\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_channels, out_channels, 4, 1, 0),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.PReLU()\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.Upsample(scale_factor=scale_factor, mode='nearest'),\n",
    "                nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.PReLU()\n",
    "            )\n",
    "        self.to_rgb = nn.Conv2d(out_channels, 1, 1)  # 1 channel for grayscale\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        rgb = self.to_rgb(x)\n",
    "        return x, rgb\n",
    "\n",
    "class DiscriminatorBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, initial_block=False):\n",
    "        super(DiscriminatorBlock, self).__init__()\n",
    "        self.initial_block = initial_block\n",
    "        self.from_rgb = nn.Conv2d(1, in_channels, 1)\n",
    "        if initial_block:\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 4, 1, 0),\n",
    "                nn.LeakyReLU(0.2, inplace=True)\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 3, stride=2, padding=1),\n",
    "                nn.LeakyReLU(0.2, inplace=True)\n",
    "            )\n",
    "\n",
    "    def forward(self, x, is_raw_input=False):\n",
    "        if is_raw_input:\n",
    "            x = self.from_rgb(x)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class ProGANGenerator(nn.Module):\n",
    "    def __init__(self, latent_dim, num_classes, resolutions=[4, 8, 16, 32, 64, 112, 224]):\n",
    "        super(ProGANGenerator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.label_emb = nn.Embedding(num_classes, latent_dim)\n",
    "        scale_factors = [resolutions[i+1] / resolutions[i] for i in range(len(resolutions)-1)]\n",
    "        self.blocks = nn.ModuleList([\n",
    "            GeneratorBlock(2 * latent_dim, 512, initial_block=True),  # 4x4\n",
    "            GeneratorBlock(512, 512, scale_factor=scale_factors[0]),  # 8x8\n",
    "            GeneratorBlock(512, 256, scale_factor=scale_factors[1]),  # 16x16\n",
    "            GeneratorBlock(256, 128, scale_factor=scale_factors[2]),  # 32x32\n",
    "            GeneratorBlock(128, 64, scale_factor=scale_factors[3]),   # 64x64\n",
    "            GeneratorBlock(64, 32, scale_factor=scale_factors[4]),    # 112x112\n",
    "            GeneratorBlock(32, 16, scale_factor=scale_factors[5]),    # 224x224\n",
    "        ])\n",
    "        self.current_depth = 0\n",
    "\n",
    "    def forward(self, z, labels, alpha=1.0):\n",
    "        label_embed = self.label_emb(labels)\n",
    "        z = torch.cat([z, label_embed], dim=1)\n",
    "        z = z.view(z.size(0), z.size(1), 1, 1)\n",
    "\n",
    "        x = z\n",
    "        prev_rgb = None\n",
    "        for i in range(self.current_depth + 1):\n",
    "            x, rgb = self.blocks[i](x)\n",
    "            if i == self.current_depth and i > 0:\n",
    "                prev_rgb_up = nn.functional.interpolate(prev_rgb, size=rgb.shape[2:], mode='nearest')\n",
    "                rgb = (1 - alpha) * prev_rgb_up + alpha * rgb\n",
    "            prev_rgb = rgb\n",
    "\n",
    "        return rgb\n",
    "\n",
    "class ProGANDiscriminator(nn.Module):\n",
    "    def __init__(self, num_classes, latent_dim=100, resolutions=[4, 8, 16, 32, 64, 112, 224]):\n",
    "        super(ProGANDiscriminator, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.latent_dim = latent_dim\n",
    "        self.label_emb = nn.Embedding(num_classes, latent_dim)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            DiscriminatorBlock(16, 32),    # 224x224 -> 112x112\n",
    "            DiscriminatorBlock(32, 64),    # 112x112 -> 56x56\n",
    "            DiscriminatorBlock(64, 128),   # 56x56 -> 28x28\n",
    "            DiscriminatorBlock(128, 256),  # 28x28 -> 14x14\n",
    "            DiscriminatorBlock(256, 512),  # 14x14 -> 7x7\n",
    "            DiscriminatorBlock(512, 512),  # 8x8 -> 4x4\n",
    "            DiscriminatorBlock(512, 512, initial_block=True),  # 4x4 -> 1x1\n",
    "        ])\n",
    "        self.final_layer = nn.Linear(512 + latent_dim, 1)\n",
    "        self.current_depth = 0\n",
    "        self.resolutions = resolutions\n",
    "\n",
    "    def forward(self, img, labels, alpha=1.0, current_res=None):\n",
    "        if current_res is None:\n",
    "            raise ValueError(\"current_res must be provided\")\n",
    "\n",
    "        x = img\n",
    "        if self.current_depth > 0:\n",
    "            current_block_idx = len(self.blocks) - self.current_depth - 1\n",
    "            if alpha < 1.0:\n",
    "                # Fade-in phase\n",
    "                new_x = self.blocks[current_block_idx](x, is_raw_input=True)\n",
    "                downsampled_x = F.interpolate(x, scale_factor=0.5, mode='nearest')\n",
    "                old_x = self.blocks[current_block_idx + 1].from_rgb(downsampled_x)\n",
    "                x = alpha * new_x + (1 - alpha) * old_x\n",
    "            else:\n",
    "                # Stabilization phase\n",
    "                x = self.blocks[current_block_idx](x, is_raw_input=True)\n",
    "        else:\n",
    "            # Depth 0: Apply only the initial block\n",
    "            x = self.blocks[-1](x, is_raw_input=True)\n",
    "\n",
    "        # Apply remaining blocks\n",
    "        for i in range(len(self.blocks) - self.current_depth, len(self.blocks)):\n",
    "            x = self.blocks[i](x, is_raw_input=False)\n",
    "\n",
    "        # Final processing\n",
    "        x = x.view(x.size(0), -1)\n",
    "        label_embed = self.label_emb(labels).view(labels.size(0), -1)\n",
    "        x = torch.cat([x, label_embed], dim=1)\n",
    "        x = self.final_layer(x)\n",
    "        return x\n",
    "\n",
    "class MURADataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert('L')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6444e7a5-3bc1-4964-bf8e-618906b1a21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_penalty(discriminator, real_samples, fake_samples, labels, device, current_res):\n",
    "    alpha = torch.rand(real_samples.size(0), 1, 1, 1, device=device)\n",
    "    interpolates = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)\n",
    "    d_interpolates = discriminator(interpolates, labels, alpha=1.0, current_res=current_res)\n",
    "    fake = torch.ones(real_samples.size(0), 1, device=device, requires_grad=False)\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b1e858-dd00-4e82-a460-18b27a4dfdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_progan(generator, discriminator, dataloader, num_epochs_per_resolution, latent_dim, device, \n",
    "                 save_interval=10, save_dir=\"checkpoints\", minority_label=1, category=\"unknown\", start_depth=0):\n",
    "    g_optimizer = optim.Adam(generator.parameters(), lr=0.002, betas=(0, 0.99))  # Reduced LR\n",
    "    d_optimizer = optim.Adam(discriminator.parameters(), lr=0.002, betas=(0, 0.99))  # Reduced LR\n",
    "    lambda_gp = 25\n",
    "    n_critic = 5  # Number of discriminator updates per generator update\n",
    "    resolutions = [4, 8, 16, 32, 64, 112, 224]\n",
    "\n",
    "    for depth in range(start_depth, len(resolutions)):\n",
    "        generator.current_depth = depth\n",
    "        discriminator.current_depth = depth\n",
    "        resolution = resolutions[depth]\n",
    "        print(f\"\\nTraining at resolution {resolution}x{resolution}\")\n",
    "\n",
    "        current_transform = transforms.Compose([\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.Resize((resolution, resolution)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "        ])\n",
    "        dataset = MURADataset(dataloader.dataset.image_paths, dataloader.dataset.labels, transform=current_transform)\n",
    "        current_dataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=0)\n",
    "        print(f\"Dataset size: {len(dataset)}, Batches: {len(current_dataloader)}\")\n",
    "\n",
    "        # Fade-in phase\n",
    "        for alpha_step, alpha in enumerate(np.linspace(0, 1, num_epochs_per_resolution // 2)):\n",
    "            alpha = float(alpha)\n",
    "            print(f\"Fade-in phase, alpha={alpha:.4f}\")\n",
    "            running_d_loss = 0.0\n",
    "            running_g_loss = 0.0\n",
    "            for i, (real_imgs, labels) in enumerate(current_dataloader):\n",
    "                real_imgs = real_imgs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                batch_size = real_imgs.size(0)\n",
    "\n",
    "                # Discriminator update\n",
    "                for _ in range(n_critic):\n",
    "                    d_optimizer.zero_grad()\n",
    "                    z = torch.randn(batch_size, latent_dim, device=device)\n",
    "                    fake_labels = torch.full((batch_size,), minority_label, dtype=torch.long, device=device)\n",
    "                    fake_imgs = generator(z, fake_labels, alpha=alpha)\n",
    "\n",
    "                    real_validity = discriminator(real_imgs, labels, alpha=alpha, current_res=resolution)\n",
    "                    fake_validity = discriminator(fake_imgs.detach(), fake_labels, alpha=alpha, current_res=resolution)\n",
    "                    gradient_penalty = compute_gradient_penalty(discriminator, real_imgs, fake_imgs.detach(), \n",
    "                                                              labels, device, current_res=resolution)\n",
    "                    d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gradient_penalty\n",
    "                    d_loss = torch.clamp(d_loss, -10, 10)  # Clip to prevent extreme values\n",
    "                    d_loss.backward()\n",
    "                    d_optimizer.step()\n",
    "                    running_d_loss += d_loss.item()\n",
    "\n",
    "                # Generator update\n",
    "                g_optimizer.zero_grad()\n",
    "                fake_imgs = generator(z, fake_labels, alpha=alpha)\n",
    "                fake_validity = discriminator(fake_imgs, fake_labels, alpha=alpha, current_res=resolution)\n",
    "                g_loss = -torch.mean(fake_validity)\n",
    "                g_loss = torch.clamp(g_loss, -100, 100)  # Clip to prevent extreme values\n",
    "                g_loss.backward()\n",
    "                g_optimizer.step()\n",
    "                running_g_loss += g_loss.item()\n",
    "\n",
    "            avg_d_loss = running_d_loss / (len(current_dataloader) * n_critic)\n",
    "            avg_g_loss = running_g_loss / len(current_dataloader)\n",
    "            print(f\"Alpha {alpha:.4f}, Avg D Loss: {avg_d_loss:.4f}, Avg G Loss: {avg_g_loss:.4f}\")\n",
    "\n",
    "        # Stabilization phase\n",
    "        for epoch in range(num_epochs_per_resolution // 2):\n",
    "            running_d_loss = 0.0\n",
    "            running_g_loss = 0.0\n",
    "            for i, (real_imgs, labels) in enumerate(current_dataloader):\n",
    "                real_imgs = real_imgs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                batch_size = real_imgs.size(0)\n",
    "\n",
    "                # Discriminator update\n",
    "                for _ in range(n_critic):\n",
    "                    d_optimizer.zero_grad()\n",
    "                    z = torch.randn(batch_size, latent_dim, device=device)\n",
    "                    fake_labels = torch.full((batch_size,), minority_label, dtype=torch.long, device=device)\n",
    "                    fake_imgs = generator(z, fake_labels, alpha=1.0)\n",
    "\n",
    "                    real_validity = discriminator(real_imgs, labels, alpha=1.0, current_res=resolution)\n",
    "                    fake_validity = discriminator(fake_imgs.detach(), fake_labels, alpha=1.0, current_res=resolution)\n",
    "                    gradient_penalty = compute_gradient_penalty(discriminator, real_imgs, fake_imgs.detach(), \n",
    "                                                              labels, device, current_res=resolution)\n",
    "                    d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gradient_penalty\n",
    "                    d_loss = torch.clamp(d_loss, -10, 10)  # Clip to prevent extreme values\n",
    "                    d_loss.backward()\n",
    "                    d_optimizer.step()\n",
    "                    running_d_loss += d_loss.item()\n",
    "\n",
    "                # Generator update\n",
    "                g_optimizer.zero_grad()\n",
    "                fake_imgs = generator(z, fake_labels, alpha=1.0)\n",
    "                fake_validity = discriminator(fake_imgs, fake_labels, alpha=1.0, current_res=resolution)\n",
    "                g_loss = -torch.mean(fake_validity)\n",
    "                g_loss = torch.clamp(g_loss, -100, 100)  # Clip to prevent extreme values\n",
    "                g_loss.backward()\n",
    "                g_optimizer.step()\n",
    "                running_g_loss += g_loss.item()\n",
    "\n",
    "            avg_d_loss = running_d_loss / (len(current_dataloader) * n_critic)\n",
    "            avg_g_loss = running_g_loss / len(current_dataloader)\n",
    "            print(f\"Depth {depth}, Epoch {epoch+1}/{num_epochs_per_resolution//2}, Avg D Loss: {avg_d_loss:.4f}, Avg G Loss: {avg_g_loss:.4f}\")\n",
    "\n",
    "            if (epoch + 1) % save_interval == 0:\n",
    "                with torch.no_grad():\n",
    "                    sample_z = torch.randn(5, latent_dim, device=device)\n",
    "                    sample_labels = torch.full((5,), minority_label, dtype=torch.long, device=device)\n",
    "                    sample_imgs = generator(sample_z, sample_labels, alpha=1.0)\n",
    "                    sample_imgs = (sample_imgs * 0.5 + 0.5) * 255\n",
    "                    sample_imgs = sample_imgs.cpu().numpy().astype(np.uint8)\n",
    "                    for j in range(5):\n",
    "                        Image.fromarray(sample_imgs[j, 0], mode='L').save(\n",
    "                            f\"MURA-v1.1/synthetic/samples/{category}_{'positive' if minority_label == 1 else 'negative'}_depth{depth}_epoch{epoch+1}_{j}.png\"\n",
    "                        )\n",
    "\n",
    "        torch.save({\n",
    "            'depth': depth,\n",
    "            'generator_state_dict': generator.state_dict(),\n",
    "            'discriminator_state_dict': discriminator.state_dict(),\n",
    "            'g_optimizer_state_dict': g_optimizer.state_dict(),\n",
    "            'd_optimizer_state_dict': d_optimizer.state_dict(),\n",
    "        }, os.path.join(save_dir, f\"progan_{category}_{'positive' if minority_label == 1 else 'negative'}_depth{depth}.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb34845-aa13-4b2d-abae-7b4f2ee4b65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "latent_dim = 100\n",
    "num_epochs_per_resolution = 100\n",
    "batch_size = 16\n",
    "save_interval = 10\n",
    "checkpoint_dir = \"MURA-v1.1/checkpoints\"\n",
    "generation_batch_size = 64\n",
    "\n",
    "# Dictionary to store synthetic image paths\n",
    "synthetic_data = []\n",
    "\n",
    "# For each category, balance to target count with equal positive/negative\n",
    "for category in categories:\n",
    "    print(f\"\\nBalancing body part {category} to target count {target_count}...\")\n",
    "    \n",
    "    category_data = df_train_images[df_train_images[\"category\"] == category]\n",
    "    positive_count = len(category_data[category_data[\"label\"] == 1])\n",
    "    negative_count = len(category_data[category_data[\"label\"] == 0])\n",
    "    print(f\"Current Positive: {positive_count}, Negative: {negative_count}\")\n",
    "\n",
    "    target_positive = target_negative = target_count // 2\n",
    "    current_total = positive_count + negative_count\n",
    "\n",
    "    if current_total >= target_count:\n",
    "        if positive_count > target_positive:\n",
    "            num_synthetic_positive = 0\n",
    "            num_synthetic_negative = target_negative - negative_count\n",
    "        elif negative_count > target_negative:\n",
    "            num_synthetic_positive = target_positive - positive_count\n",
    "            num_synthetic_negative = 0\n",
    "        else:\n",
    "            num_synthetic_positive = target_positive - positive_count\n",
    "            num_synthetic_negative = target_negative - negative_count\n",
    "    else:\n",
    "        num_synthetic_positive = target_positive - positive_count\n",
    "        num_synthetic_negative = target_negative - negative_count\n",
    "\n",
    "    if num_synthetic_positive > 0:\n",
    "        print(f\"Generating {num_synthetic_positive} synthetic positive images for {category}...\")\n",
    "        minority_label = 1\n",
    "        minority_images = category_data[category_data[\"label\"] == minority_label][\"image_path\"].tolist()\n",
    "        minority_labels = [minority_label] * len(minority_images)\n",
    "        progan_dataset = MURADataset(minority_images, minority_labels, transform=transform)\n",
    "        progan_loader = DataLoader(progan_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "        generator = ProGANGenerator(latent_dim, num_classes=2).to(device)\n",
    "        discriminator = ProGANDiscriminator(num_classes=2, latent_dim=latent_dim).to(device)\n",
    "        g_optimizer = optim.Adam(generator.parameters(), lr=0.001, betas=(0, 0.99))\n",
    "        d_optimizer = optim.Adam(discriminator.parameters(), lr=0.001, betas=(0, 0.99))\n",
    "\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f\"progan_{category}_positive_depth{len([4, 8, 16, 32, 64, 112, 224])-1}.pt\")\n",
    "        start_depth = 0\n",
    "        if os.path.exists(checkpoint_path):\n",
    "            checkpoint = torch.load(checkpoint_path)\n",
    "            generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "            discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "            g_optimizer.load_state_dict(checkpoint['g_optimizer_state_dict'])\n",
    "            d_optimizer.load_state_dict(checkpoint['d_optimizer_state_dict'])\n",
    "            start_depth = checkpoint['depth']\n",
    "            print(f\"Resuming training for {category} (positive) from depth {start_depth}\")\n",
    "        else:\n",
    "            print(f\"No checkpoint found for {category} (positive), starting from scratch.\")\n",
    "\n",
    "        if start_depth < len([4, 8, 16, 32, 64, 112, 224]):\n",
    "            train_progan(generator, discriminator, progan_loader, num_epochs_per_resolution, latent_dim, device,\n",
    "                         save_interval=save_interval, save_dir=checkpoint_dir, \n",
    "                         minority_label=minority_label, category=category, start_depth=start_depth)\n",
    "\n",
    "        synthetic_images = []\n",
    "        synthetic_labels = [minority_label] * num_synthetic_positive\n",
    "        for i in range(0, num_synthetic_positive, generation_batch_size):\n",
    "            batch_size = min(generation_batch_size, num_synthetic_positive - i)\n",
    "            batch_labels = torch.tensor(synthetic_labels[i:i + batch_size], dtype=torch.long, device=device)\n",
    "            with torch.no_grad():\n",
    "                z = torch.randn(batch_size, latent_dim, device=device)\n",
    "                batch_images = generator(z, batch_labels, alpha=1.0)\n",
    "                synthetic_images.append(batch_images.cpu())\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        synthetic_images = torch.cat(synthetic_images, dim=0)\n",
    "        synthetic_dir = f\"MURA-v1.1/synthetic/{category}/positive\"\n",
    "        os.makedirs(synthetic_dir, exist_ok=True)\n",
    "        for i, img in enumerate(synthetic_images):\n",
    "            img = img.squeeze().numpy()\n",
    "            img = (img * 0.5) + 0.5\n",
    "            img = (img * 255).astype(np.uint8)\n",
    "            img_path = os.path.join(synthetic_dir, f\"synthetic_{i}.png\")\n",
    "            Image.fromarray(img, mode='L').save(img_path)\n",
    "            synthetic_data.append([img_path, minority_label, category])\n",
    "\n",
    "        del generator, discriminator, progan_dataset, progan_loader, synthetic_images\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    if num_synthetic_negative > 0:\n",
    "        print(f\"Generating {num_synthetic_negative} synthetic negative images for {category}...\")\n",
    "        minority_label = 0\n",
    "        minority_images = category_data[category_data[\"label\"] == minority_label][\"image_path\"].tolist()\n",
    "        minority_labels = [minority_label] * len(minority_images)\n",
    "        progan_dataset = MURADataset(minority_images, minority_labels, transform=transform)\n",
    "        progan_loader = DataLoader(progan_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "        generator = ProGANGenerator(latent_dim, num_classes=2).to(device)\n",
    "        discriminator = ProGANDiscriminator(num_classes=2, latent_dim=latent_dim).to(device)\n",
    "        g_optimizer = optim.Adam(generator.parameters(), lr=0.001, betas=(0, 0.99))\n",
    "        d_optimizer = optim.Adam(discriminator.parameters(), lr=0.001, betas=(0, 0.99))\n",
    "\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f\"progan_{category}_negative_depth{len([4, 8, 16, 32, 64, 112, 224])-1}.pt\")\n",
    "        start_depth = 0\n",
    "        if os.path.exists(checkpoint_path):\n",
    "            checkpoint = torch.load(checkpoint_path)\n",
    "            generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "            discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "            g_optimizer.load_state_dict(checkpoint['g_optimizer_state_dict'])\n",
    "            d_optimizer.load_state_dict(checkpoint['d_optimizer_state_dict'])\n",
    "            start_depth = checkpoint['depth']\n",
    "            print(f\"Resuming training for {category} (negative) from depth {start_depth}\")\n",
    "        else:\n",
    "            print(f\"No checkpoint found for {category} (negative), starting from scratch.\")\n",
    "\n",
    "        if start_depth < len([4, 8, 16, 32, 64, 112, 224]):\n",
    "            train_progan(generator, discriminator, progan_loader, num_epochs_per_resolution, latent_dim, device,\n",
    "                         save_interval=save_interval, save_dir=checkpoint_dir, \n",
    "                         minority_label=minority_label, category=category, start_depth=start_depth)\n",
    "\n",
    "        synthetic_images = []\n",
    "        synthetic_labels = [minority_label] * num_synthetic_negative\n",
    "        for i in range(0, num_synthetic_negative, generation_batch_size):\n",
    "            batch_size = min(generation_batch_size, num_synthetic_negative - i)\n",
    "            batch_labels = torch.tensor(synthetic_labels[i:i + batch_size], dtype=torch.long, device=device)\n",
    "            with torch.no_grad():\n",
    "                z = torch.randn(batch_size, latent_dim, device=device)\n",
    "                batch_images = generator(z, batch_labels, alpha=1.0)\n",
    "                synthetic_images.append(batch_images.cpu())\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        synthetic_images = torch.cat(synthetic_images, dim=0)\n",
    "        synthetic_dir = f\"MURA-v1.1/synthetic/{category}/negative\"\n",
    "        os.makedirs(synthetic_dir, exist_ok=True)\n",
    "        for i, img in enumerate(synthetic_images):\n",
    "            img = img.squeeze().numpy()\n",
    "            img = (img * 0.5) + 0.5\n",
    "            img = (img * 255).astype(np.uint8)\n",
    "            img_path = os.path.join(synthetic_dir, f\"synthetic_{i}.png\")\n",
    "            Image.fromarray(img, mode='L').save(img_path)\n",
    "            synthetic_data.append([img_path, minority_label, category])\n",
    "\n",
    "        del generator, discriminator, progan_dataset, progan_loader, synthetic_images\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "print(\"Positive/Negative balancing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb262a9-2223-4465-ad41-e729887ba25c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

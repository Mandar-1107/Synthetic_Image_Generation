{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a662debb-7590-4e5e-b7b9-d42bf086fa04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "\n",
    "# Check CUDA availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Define constants\n",
    "IMG_SIZE = 512\n",
    "LATENT_DIM = 512\n",
    "NUM_CLASSES = 14\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 100\n",
    "LAMBDA_GP = 15  \n",
    "DATASET_DIR = 'MURA-v1.1/train'\n",
    "CHECKPOINT_DIR = 'checkpoints-BAGAN-GP-WGAN-GP_Old-2'\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Define class names\n",
    "body_parts = ['XR_WRIST', 'XR_SHOULDER', 'XR_HAND', 'XR_FOREARM', 'XR_FINGER', 'XR_ELBOW', 'XR_HUMERUS']\n",
    "case_types = ['positive', 'negative']\n",
    "class_names = [f\"{bp}_{ct}\" for bp in body_parts for ct in case_types]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30b7be4c-a29b-470b-941a-af1c06274401",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MURADataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            img_path = self.image_paths[idx]\n",
    "            full_path = os.path.join(DATASET_DIR, '..', img_path)\n",
    "            image = Image.open(full_path).convert('L')  # Grayscale for X-rays\n",
    "            \n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            \n",
    "            label = self.labels[idx]\n",
    "            label_onehot = torch.zeros(NUM_CLASSES)\n",
    "            label_onehot[label] = 1.0\n",
    "            \n",
    "            return image, label_onehot\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            placeholder = torch.zeros((1, IMG_SIZE, IMG_SIZE))\n",
    "            return placeholder, torch.zeros(NUM_CLASSES)\n",
    "\n",
    "# Helper Functions for Dataset Processing\n",
    "def get_class_label(image_path):\n",
    "    for bp in body_parts:\n",
    "        if bp in image_path:\n",
    "            case = 'positive' if 'positive' in image_path else 'negative'\n",
    "            class_idx = body_parts.index(bp) * 2 + (0 if case == 'positive' else 1)\n",
    "            return class_idx\n",
    "    return None\n",
    "\n",
    "def scan_dataset(dataset_dir=DATASET_DIR):\n",
    "    print(f\"Scanning MURA dataset directory: {dataset_dir}...\")\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "\n",
    "    for root, dirs, files in os.walk(dataset_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.png'):\n",
    "                full_path = os.path.join(root, file)\n",
    "                relative_path = os.path.relpath(full_path, os.path.join(dataset_dir, '..'))\n",
    "                image_paths.append(relative_path)\n",
    "                labels.append(1 if 'positive' in relative_path else 0)\n",
    "\n",
    "    image_paths = np.array(image_paths)\n",
    "    labels = np.array(labels)\n",
    "    print(f\"Found {len(image_paths)} images with labels\")\n",
    "\n",
    "    print(\"Classifying images...\")\n",
    "    class_labels = []\n",
    "    valid_paths = []\n",
    "\n",
    "    for path in tqdm(image_paths):\n",
    "        label = get_class_label(path)\n",
    "        if label is not None:\n",
    "            class_labels.append(label)\n",
    "            valid_paths.append(path)\n",
    "\n",
    "    valid_paths = np.array(valid_paths)\n",
    "    class_labels = np.array(class_labels)\n",
    "    print(f\"Successfully classified {len(class_labels)} images into classes\")\n",
    "\n",
    "    class_counts = Counter(class_labels)\n",
    "    print(\"Class distribution:\")\n",
    "    for cls in sorted(class_counts.keys()):\n",
    "        print(f\"{class_names[cls]}: {class_counts[cls]} images\")\n",
    "    \n",
    "    return valid_paths, class_labels, class_counts  # Fixed return statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dd1a6b1-e559-4703-b0d6-5564320dc8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.spectral_norm as spectral_norm  # Added for spectral normalization\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 4, stride=2, padding=1),  # 512 -> 256\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 256 -> 128\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, 4, stride=2, padding=1),  # 128 -> 64\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 512, 4, stride=2, padding=1),  # 64 -> 32\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(512, 1024, 4, stride=2, padding=1),  # 32 -> 16\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        # Corrected dimension calculation: 1024 * 16 * 16\n",
    "        self.fc = nn.Linear(1024 * 16 * 16, LATENT_DIM)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        latent = self.fc(x)\n",
    "        return latent\n",
    "\n",
    "# Decoder/Generator Network\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.initial_size = 16\n",
    "        self.fc = nn.Linear(LATENT_DIM + NUM_CLASSES, 1024 * self.initial_size * self.initial_size)\n",
    "        \n",
    "        # Change this line to match the checkpoint\n",
    "        self.bn_initial = nn.BatchNorm2d(1024)\n",
    "        \n",
    "        self.deconv = nn.Sequential(\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.ConvTranspose2d(1024, 512, 4, stride=2, padding=1),  # 16 -> 32\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1),  # 32 -> 64\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),  # 64 -> 128\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # 128 -> 256\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.ConvTranspose2d(64, 1, 4, stride=2, padding=1),  # 256 -> 512\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, z, labels):\n",
    "        x = torch.cat([z, labels], dim=1)\n",
    "        x = self.fc(x)\n",
    "        x = x.view(-1, 1024, self.initial_size, self.initial_size)\n",
    "        x = self.bn_initial(x)  # Use bn_initial instead of init_block\n",
    "        x = nn.functional.relu(x, inplace=True)  # Apply ReLU separately\n",
    "        x = self.deconv(x)\n",
    "        return x\n",
    "\n",
    "# Discriminator Network\n",
    "from torch.nn.utils import spectral_norm\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            spectral_norm(nn.Conv2d(1, 32, 4, stride=2, padding=1)),  # 512 -> 256\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            spectral_norm(nn.Conv2d(32, 64, 4, stride=2, padding=1)),  # 256 -> 128\n",
    "            nn.InstanceNorm2d(64, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            spectral_norm(nn.Conv2d(64, 128, 4, stride=2, padding=1)),  # 128 -> 64\n",
    "            nn.InstanceNorm2d(128, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            spectral_norm(nn.Conv2d(128, 256, 4, stride=2, padding=1)),  # 64 -> 32\n",
    "            nn.InstanceNorm2d(256, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            spectral_norm(nn.Conv2d(256, 512, 4, stride=2, padding=1)),  # 32 -> 16\n",
    "            nn.InstanceNorm2d(512, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.embed_dim = 512\n",
    "        self.class_embedding = nn.Linear(num_classes, self.embed_dim)\n",
    "        \n",
    "        ds_size = 16\n",
    "        self.feature_size = 512 * ds_size * ds_size\n",
    "        self.adv_layer = nn.Linear(self.feature_size + self.embed_dim, 1)\n",
    "        self.aux_layer = nn.Linear(self.feature_size + self.embed_dim, num_classes)\n",
    "    \n",
    "    def forward(self, img, labels):\n",
    "        features = self.main(img)\n",
    "        features = self.flatten(features)\n",
    "        \n",
    "        label_embedding = self.class_embedding(labels)\n",
    "        \n",
    "        combined = torch.cat([features, label_embedding], dim=1)\n",
    "        \n",
    "        validity = self.adv_layer(combined)\n",
    "        label_logits = self.aux_layer(combined)\n",
    "        \n",
    "        return validity, label_logits\n",
    "\n",
    "# Autoencoder (Encoder + Decoder) Network\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        self.num_classes = num_classes\n",
    "    \n",
    "    def forward(self, x, labels):\n",
    "        latent = self.encoder(x)\n",
    "        reconstructed = self.decoder(latent, labels)\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b096196-fcd1-47a9-97f9-95f5b3a39b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import vgg16\n",
    "vgg = vgg16(pretrained=True).features[:16].to(device)  # Up to the 5th conv block\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self, vgg_model):\n",
    "        super(PerceptualLoss, self).__init__()\n",
    "        self.vgg = vgg_model\n",
    "        self.mse = nn.MSELoss()\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        x_vgg = self.vgg(x.repeat(1, 3, 1, 1))  # Repeat grayscale to 3 channels for VGG\n",
    "        y_vgg = self.vgg(y.repeat(1, 3, 1, 1))\n",
    "        return self.mse(x_vgg, y_vgg)\n",
    "\n",
    "def pretrain_autoencoder(dataloader, val_dataloader, num_classes, epochs=50, device='cuda', checkpoint_dir='checkpoints'):\n",
    "    # Ensure checkpoint directory exists\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # Update LATENT_DIM to 512\n",
    "    global LATENT_DIM\n",
    "    LATENT_DIM = 512\n",
    "    \n",
    "    # Initialize autoencoder with updated latent dimension\n",
    "    autoencoder = Autoencoder(num_classes).to(device)\n",
    "    optimizer = optim.Adam(autoencoder.parameters(), lr=0.0008, betas=(0.5, 0.999))\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "    pixel_loss = nn.MSELoss()\n",
    "    perceptual_loss = PerceptualLoss(vgg)\n",
    "    \n",
    "    # Find the latest checkpoint\n",
    "    checkpoint_files = glob(os.path.join(checkpoint_dir, 'autoencoder_epoch_*.pth'))\n",
    "    start_epoch = 0\n",
    "    best_val_loss = float('inf')\n",
    "    if checkpoint_files:\n",
    "        # Select checkpoint with highest epoch number\n",
    "        epoch_numbers = [int(os.path.basename(f).split('_')[-1].replace('.pth', '')) for f in checkpoint_files]\n",
    "        max_epoch = max(epoch_numbers)\n",
    "        latest_checkpoint = os.path.join(checkpoint_dir, f'autoencoder_epoch_{max_epoch}.pth')\n",
    "        print(f\"Resuming from checkpoint: {latest_checkpoint} at epoch {max_epoch + 1}\")\n",
    "        autoencoder.load_state_dict(torch.load(latest_checkpoint, map_location=device))\n",
    "        start_epoch = max_epoch\n",
    "    else:\n",
    "        print(\"No checkpoint found, starting from epoch 1\")\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        autoencoder.train()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(dataloader, desc=f\"Autoencoder Epoch {epoch+1}/{epochs}\"):\n",
    "            images, labels = batch\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            reconstructed = autoencoder(images, labels)\n",
    "            mse_loss = pixel_loss(reconstructed, images)\n",
    "            perc_loss = perceptual_loss(reconstructed, images)\n",
    "            loss = mse_loss + 0.1 * perc_loss  # Balance MSE and perceptual loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Autoencoder Epoch [{epoch+1}/{epochs}], Train Loss: {avg_loss:.4f} (MSE: {mse_loss.item():.4f}, Perceptual: {perc_loss.item():.4f})\")\n",
    "        \n",
    "        # Validation\n",
    "        autoencoder.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                images, labels = batch\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                reconstructed = autoencoder(images, labels)\n",
    "                val_loss += pixel_loss(reconstructed, images).item()\n",
    "        avg_val_loss = val_loss / len(val_dataloader)\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # Save checkpoint if validation loss improves\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(autoencoder.state_dict(), os.path.join(checkpoint_dir, 'best_autoencoder.pth'))\n",
    "            print(f\"Best checkpoint saved: {os.path.join(checkpoint_dir, 'best_autoencoder.pth')}\")\n",
    "        \n",
    "        # Save regular checkpoint\n",
    "        torch.save(autoencoder.state_dict(), os.path.join(checkpoint_dir, f'autoencoder_epoch_{epoch+1}.pth'))\n",
    "        print(f\"Checkpoint saved: {os.path.join(checkpoint_dir, f'autoencoder_epoch_{epoch+1}.pth')}\")\n",
    "        \n",
    "        scheduler.step()  # Update learning rate\n",
    "    \n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a61fcfc6-77a9-4d1a-b3dd-f5eb8fa3acc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_penalty(discriminator, real_images, fake_images, labels):\n",
    "    batch_size = real_images.size(0)\n",
    "    alpha = torch.rand(batch_size, 1, 1, 1, device=device)\n",
    "    interpolates = (alpha * real_images + (1 - alpha) * fake_images).requires_grad_(True)\n",
    "    \n",
    "    # Pass interpolates through discriminator\n",
    "    disc_interpolates, _ = discriminator(interpolates, labels)\n",
    "    \n",
    "    # Get gradients\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=disc_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=torch.ones_like(disc_interpolates),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    if torch.isnan(gradients).any() or torch.isinf(gradients).any():\n",
    "        print(\"NaN or Inf gradients detected!\")\n",
    "        \n",
    "    # Calculate penalty\n",
    "    gradients = gradients.view(batch_size, -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * LAMBDA_GP\n",
    "    return gradient_penalty\n",
    "\n",
    "def save_checkpoint(epoch, generator, discriminator, gen_optimizer, disc_optimizer, d_losses, g_losses):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'generator_state_dict': generator.state_dict(),\n",
    "        'discriminator_state_dict': discriminator.state_dict(),\n",
    "        'gen_optimizer_state_dict': gen_optimizer.state_dict(),\n",
    "        'disc_optimizer_state_dict': disc_optimizer.state_dict(),\n",
    "        'd_losses': d_losses,\n",
    "        'g_losses': g_losses\n",
    "    }\n",
    "    checkpoint_path = os.path.join(CHECKPOINT_DIR, f'checkpoint_epoch_{epoch}.pth')\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "def load_checkpoint(checkpoint_path, generator, discriminator, gen_optimizer, disc_optimizer):\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "        discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "        gen_optimizer.load_state_dict(checkpoint['gen_optimizer_state_dict'])\n",
    "        disc_optimizer.load_state_dict(checkpoint['disc_optimizer_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        d_losses = checkpoint['d_losses']\n",
    "        g_losses = checkpoint['g_losses']\n",
    "        print(f\"Loaded checkpoint from {checkpoint_path}, resuming from epoch {epoch + 1}\")\n",
    "        return epoch, d_losses, g_losses\n",
    "    else:\n",
    "        print(f\"No checkpoint found at {checkpoint_path}\")\n",
    "        return 0, [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3efc036c-4eca-49a9-a403-d17e1d42b6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WGAN-GP BAGAN Training Function\n",
    "def train_wgan_gp_bagan(dataloader, generator, discriminator, num_epochs, device, checkpoint_dir, resume_epoch=0):\n",
    "    # Optimizers\n",
    "    disc_optimizer = optim.Adam(discriminator.parameters(), lr=0.0001, betas=(0.5, 0.9))\n",
    "    gen_optimizer = optim.Adam(generator.parameters(), lr=0.0008, betas=(0.5, 0.9))\n",
    "    \n",
    "    # Loss tracking\n",
    "    d_losses = []\n",
    "    g_losses = []\n",
    "    \n",
    "    # Resume training if specified\n",
    "    start_epoch = 0\n",
    "    if resume_epoch is not None:\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{resume_epoch}.pth')\n",
    "        start_epoch, d_losses, g_losses = load_checkpoint(\n",
    "            checkpoint_path, generator, discriminator, gen_optimizer, disc_optimizer\n",
    "        )\n",
    "    \n",
    "    # Classification loss for auxiliary classifier\n",
    "    classification_loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        generator.train()\n",
    "        discriminator.train()\n",
    "        total_d_loss = 0\n",
    "        total_g_loss = 0\n",
    "        \n",
    "        for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            real_images, labels_onehot = batch\n",
    "            real_images = real_images.to(device)\n",
    "            labels_onehot = labels_onehot.to(device)\n",
    "            labels = torch.argmax(labels_onehot, dim=1)\n",
    "            batch_size = real_images.size(0)\n",
    "            \n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "            for _ in range(1):  # Multiple D updates per G update for better stability\n",
    "                disc_optimizer.zero_grad()\n",
    "                \n",
    "                # Process real images\n",
    "                real_validity, real_label_logits = discriminator(real_images, labels_onehot)\n",
    "                d_loss_real = -torch.mean(real_validity)\n",
    "                class_loss_real = classification_loss(real_label_logits, labels)\n",
    "                \n",
    "                # Generate fake images\n",
    "                noise = torch.randn(batch_size, LATENT_DIM, device=device)\n",
    "                fake_labels_idx = torch.randint(0, NUM_CLASSES, (batch_size,), device=device)\n",
    "                fake_labels_onehot = torch.zeros(batch_size, NUM_CLASSES, device=device)\n",
    "                fake_labels_onehot.scatter_(1, fake_labels_idx.unsqueeze(1), 1)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    fake_images = generator(noise, fake_labels_onehot)\n",
    "                \n",
    "                # Process fake images \n",
    "                fake_validity, _ = discriminator(fake_images, fake_labels_onehot)\n",
    "                d_loss_fake = torch.mean(fake_validity)\n",
    "                \n",
    "                # Gradient penalty\n",
    "                gp = compute_gradient_penalty(discriminator, real_images, fake_images, labels_onehot)\n",
    "                \n",
    "                # Total discriminator loss\n",
    "                d_loss = d_loss_real + d_loss_fake + gp + class_loss_real\n",
    "                d_loss.backward()\n",
    "                disc_optimizer.step()\n",
    "                \n",
    "                total_d_loss += d_loss.item()\n",
    "            \n",
    "            # -----------------\n",
    "            #  Train Generator\n",
    "            # -----------------\n",
    "            for _ in range(1): \n",
    "                gen_optimizer.zero_grad()\n",
    "                \n",
    "                # Generate fake images\n",
    "                noise = torch.randn(batch_size, LATENT_DIM, device=device)\n",
    "                gen_labels_idx = torch.randint(0, NUM_CLASSES, (batch_size,), device=device)\n",
    "                gen_labels_onehot = torch.zeros(batch_size, NUM_CLASSES, device=device)\n",
    "                gen_labels_onehot.scatter_(1, gen_labels_idx.unsqueeze(1), 1)\n",
    "                \n",
    "                fake_images = generator(noise, gen_labels_onehot)\n",
    "                \n",
    "                # Process fake images with discriminator\n",
    "                fake_validity, fake_label_logits = discriminator(fake_images, gen_labels_onehot)\n",
    "                \n",
    "                # Generator losses\n",
    "                g_loss_adv = -torch.mean(fake_validity)\n",
    "                class_loss_fake = classification_loss(fake_label_logits, gen_labels_idx)\n",
    "                \n",
    "                # Total generator loss\n",
    "                g_loss = g_loss_adv + class_loss_fake\n",
    "                g_loss.backward()\n",
    "                gen_optimizer.step()\n",
    "                \n",
    "                total_g_loss += g_loss.item()\n",
    "        \n",
    "        # End of epoch\n",
    "        avg_d_loss = total_d_loss / len(dataloader)\n",
    "        avg_g_loss = total_g_loss / len(dataloader)\n",
    "        d_losses.append(avg_d_loss)\n",
    "        g_losses.append(avg_g_loss)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], D Loss: {avg_d_loss:.4f} (Real: {d_loss_real.item():.4f}, Fake: {d_loss_fake.item():.4f}, GP: {gp.item():.4f}, Class: {class_loss_real.item():.4f}), G Loss: {avg_g_loss:.4f} (Adv: {g_loss_adv.item():.4f}, Class: {class_loss_fake.item():.4f})\")\n",
    "        \n",
    "        # Save checkpoint and generate samples\n",
    "        if (epoch + 1) % 1 == 0:\n",
    "            save_checkpoint(epoch + 1, generator, discriminator, gen_optimizer, disc_optimizer, d_losses, g_losses)\n",
    "            \n",
    "            # Generate and save sample images\n",
    "            with torch.no_grad():\n",
    "                noise = torch.randn(NUM_CLASSES, LATENT_DIM, device=device)\n",
    "                sample_labels_onehot = torch.eye(NUM_CLASSES, device=device)\n",
    "                fake_images = generator(noise, sample_labels_onehot)\n",
    "                fake_images = (fake_images + 1) / 2  # Scale from [-1,1] to [0,1]\n",
    "                \n",
    "                # Create grid of sample images\n",
    "                fig, axes = plt.subplots(2, 7, figsize=(14, 4))\n",
    "                axes = axes.flatten()\n",
    "                \n",
    "                for i in range(NUM_CLASSES):\n",
    "                    img = fake_images[i].cpu().numpy().squeeze()\n",
    "                    axes[i].imshow(img, cmap='gray')\n",
    "                    axes[i].set_title(class_names[i])\n",
    "                    axes[i].axis('off')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(checkpoint_dir, f'samples_epoch_{epoch+1}.png'))\n",
    "                plt.close()\n",
    "                \n",
    "                # Save individual images\n",
    "                for i in range(NUM_CLASSES):\n",
    "                    img = fake_images[i].cpu().numpy().squeeze()\n",
    "                    plt.imsave(os.path.join(checkpoint_dir, f'epoch_{epoch+1}_class_{i}.png'), img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4ed0e35-0823-498a-8531-350bd710635a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Generate Balanced Dataset\n",
    "def generate_balanced_dataset(generator, class_counts, class_names, num_classes, latent_dim, device, output_dir):\n",
    "    \"\"\"\n",
    "    Generate synthetic images to balance the dataset.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Calculate target counts for balanced dataset\n",
    "    total_images = sum(class_counts.values())\n",
    "    target_per_body_part = total_images / 7  # Equal distribution across 7 body parts\n",
    "    target_per_class = target_per_body_part / 2  # 50/50 split between positive/negative\n",
    "    \n",
    "    print(f\"Total images: {total_images}\")\n",
    "    print(f\"Target per body part: {target_per_body_part:.0f}\")\n",
    "    print(f\"Target per class: {target_per_class:.0f}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for body_part_idx in range(0, num_classes, 2):\n",
    "            body_part_name = class_names[body_part_idx].split('_')[0]\n",
    "            positive_class = body_part_idx\n",
    "            negative_class = body_part_idx + 1\n",
    "            \n",
    "            current_positive = class_counts.get(positive_class, 0)\n",
    "            current_negative = class_counts.get(negative_class, 0)\n",
    "            \n",
    "            images_needed_positive = max(0, int(target_per_class) - current_positive)\n",
    "            images_needed_negative = max(0, int(target_per_class) - current_negative)\n",
    "            \n",
    "            print(f\"Balancing {body_part_name}:\")\n",
    "            print(f\"  Positive: {current_positive} -> need {images_needed_positive} more\")  \n",
    "            print(f\"  Negative: {current_negative} -> need {images_needed_negative} more\")\n",
    "            \n",
    "            # Create directories\n",
    "            pos_dir = os.path.join(output_dir, class_names[positive_class])\n",
    "            neg_dir = os.path.join(output_dir, class_names[negative_class])\n",
    "            os.makedirs(pos_dir, exist_ok=True)\n",
    "            os.makedirs(neg_dir, exist_ok=True)\n",
    "            \n",
    "            # Generate positive class images\n",
    "            if images_needed_positive > 0:\n",
    "                batch_size = 16\n",
    "                num_batches = (images_needed_positive + batch_size - 1) // batch_size\n",
    "                \n",
    "                for batch in tqdm(range(num_batches), desc=f\"Generating {body_part_name} positive\"):\n",
    "                    current_batch_size = min(batch_size, images_needed_positive - batch * batch_size)\n",
    "                    if current_batch_size <= 0:\n",
    "                        break\n",
    "                        \n",
    "                    noise = torch.randn(current_batch_size, latent_dim, device=device)\n",
    "                    labels_onehot = torch.zeros(current_batch_size, num_classes, device=device)\n",
    "                    labels_onehot[:, positive_class] = 1.0\n",
    "                    \n",
    "                    fake_images = generator(noise, labels_onehot)\n",
    "                    fake_images = (fake_images + 1) / 2  # Scale from [-1,1] to [0,1]\n",
    "                    \n",
    "                    for i in range(current_batch_size):\n",
    "                        img = fake_images[i].cpu().numpy().squeeze()\n",
    "                        img = (img * 255).astype(np.uint8)\n",
    "                        img_pil = Image.fromarray(img, mode='L')\n",
    "                        img_pil.save(os.path.join(pos_dir, f\"synthetic_{batch * batch_size + i}.png\"))\n",
    "            \n",
    "            # Generate negative class images\n",
    "            if images_needed_negative > 0:\n",
    "                batch_size = 16\n",
    "                num_batches = (images_needed_negative + batch_size - 1) // batch_size\n",
    "                \n",
    "                for batch in tqdm(range(num_batches), desc=f\"Generating {body_part_name} negative\"):\n",
    "                    current_batch_size = min(batch_size, images_needed_negative - batch * batch_size)\n",
    "                    if current_batch_size <= 0:\n",
    "                        break\n",
    "                        \n",
    "                    noise = torch.randn(current_batch_size, latent_dim, device=device)\n",
    "                    labels_onehot = torch.zeros(current_batch_size, num_classes, device=device)\n",
    "                    labels_onehot[:, negative_class] = 1.0\n",
    "                    \n",
    "                    fake_images = generator(noise, labels_onehot)\n",
    "                    fake_images = (fake_images + 1) / 2  # Scale from [-1,1] to [0,1]\n",
    "                    \n",
    "                    for i in range(current_batch_size):\n",
    "                        img = fake_images[i].cpu().numpy().squeeze()\n",
    "                        img = (img * 255).astype(np.uint8)\n",
    "                        img_pil = Image.fromarray(img, mode='L')\n",
    "                        img_pil.save(os.path.join(neg_dir, f\"synthetic_{batch * batch_size + i}.png\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ec2e7c-6b97-43d3-bcfc-704c9a7aff1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare training data\n",
    "image_paths, labels, class_counts = scan_dataset(DATASET_DIR)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize to [-1, 1] for tanh activation\n",
    "])\n",
    "dataset = MURADataset(image_paths, labels, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "\n",
    "# Load and prepare validation data\n",
    "val_image_paths, val_labels, val_class_counts = scan_dataset('MURA-v1.1/valid')\n",
    "val_dataset = MURADataset(val_image_paths, val_labels, transform=transform)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "# Pretrain Autoencoder\n",
    "autoencoder = pretrain_autoencoder(dataloader, val_dataloader, NUM_CLASSES, epochs=50, device=device, checkpoint_dir=CHECKPOINT_DIR)\n",
    "\n",
    "# Extract the pretrained decoder (generator) and initialize discriminator\n",
    "generator = autoencoder.decoder.to(device)\n",
    "discriminator = Discriminator(NUM_CLASSES).to(device)\n",
    "\n",
    "# Train WGAN-GP with BAGAN framework\n",
    "train_wgan_gp_bagan(dataloader, generator, discriminator, NUM_EPOCHS, device, CHECKPOINT_DIR, resume_epoch=0)\n",
    "\n",
    "# Generate balanced dataset\n",
    "output_dir = 'synthetic_mura'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9cf13c-8ae4-40aa-a1fb-841b6c8fc287",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
